---
title: "Lista 1 Métodos Computacionais para Ciência de Dados II"
author: "Emanuelle Oliveira, Maria Luiza Oliveira, Mariana Fleming"
execute: 
  echo: true
  warning: false
  message: false
output:
  pdf_document:
    keep_tex: true
  html_document: default
---

## Execício 1.4
Implemente um algoritmo para fazer permutação utilizando o algoritmo para
gerar uma amostra de uma variável categórica. Não utilize a função sample do R.

## Solução

**1.** Armazene os valores em vetor de entrada e faça $n =$ tamanho do vetor;
  
**2.** Dentro de um _loop_ onde $j$ percorre os valores de $n$ até 2, faça: 
  
- Gere uma variável $i$ tal que seja o `floor()` - função que retorna o menor inteiro - da função $(Uniforme(0,1)*j)+1$
- Em uma variável auxiliar salve o valor da posição $i$ do vetor criado no Passo 1 e troque-a com o valor da posição $j$ do vetor.
  
**3.** Retorne o vetor criado agora permutado.

```{r permutacao}
permutacao <- function(vetor){
  vetor_aux <- vetor
  n <- length(vetor_aux)
  
  for (j in n:2) {
    i <- floor(runif(1, 0, 1) * j) + 1

    temp <- vetor_aux[i]
    vetor_aux[i] <- vetor_aux[j]
    vetor_aux[j] <- temp
  }
  
  return(vetor_aux)
  
}

# Exemplo, vetor de 1 à 10
vetor <- 1:10
vetor_permutado <- permutacao(vetor)
vetor_permutado

```



## Execício 1.5
Implemente o algoritmo para gerar da distribuição geométrica.

## Solução

Código inspirado pelo livro *Statistical Computing with R, Second Edition*.
Nesse formato, estamos contabilizando o número da tentativa em que ocorre o primeiro sucesso.

**1.** Gere $n$ valores de uma $Uniforme(0,1)$
  
**2.** Seja $p$ e $q=1-p$ parâmetros da distribuição Geométrica, calculamos:
  
- $P(X=i) = pq^{i-1}, i \ge 1$
- $P(X \le j-1) =  \sum_{i=1}^{j-1}P(X=i) = 1-P(X > j-1) = 1-q^{j-1}, j \ge 1$
- Geramos *K* por gerar $U \sim Uniforme(0,1)$ e faça $K=q$: $1-q^{j-1} \le U < 1-q^j$
- Desenvolvendo essa conta, podemos ter $K = min\{j: q^j < 1-U \} = min\{j: j > \frac{log(1-U)}{log(q)} \}$
- Então teremos $Int(\frac{log(U)}{log(q)})+1$
  
**3.** Calculamos *K* utilizando a formula `floor()` e retornamos.

```{r rgeom}
rgeom <- function(n,p){
  U <- runif(n, 0, 1)
  q <- 1 - p
  
  # Tentativas até o primeiro sucesso
  K <- floor(log(U) / log(q))+1 
  return(K)
}

# Exemplo de amostra de 10000 com p=0.25 e q=0.75
n <- 10000
p <- 0.25

X <- rgeom(n, p)

# Gráfico da Amostra
hist(X,
     prob = TRUE,
     main = "Distribuição Geométrica (k >= 1)",
     xlab = "Número de Tentativas (k)",
     ylab = "Frequência Relativa")


# Linha Teórica da distriguição geométrica
x_vals <- 1:max(X)
y_vals <- dgeom(x_vals - 1, prob = p)
lines(x_vals, y_vals, col = "tomato3", lwd = 2)
```

## Execício 1.6
Implemente a versão otimizada do algoritmo para gerar da distribuição Poisson e faça comparações do tempo computacional com o algoritmo inicial. Utilize o pacote `microbenchmark` para comparar.

## Solução

Nessa solução iremos comparar 3 algoritmos para geração de amostras aleatórias da distribuição Poisson:

- O algoritmo disponível no livro-texto da matéria *Métodos Computacionais Aplicados à Estatística. Implementação no Software R.*;
- O algoritmo de *Knuth*, inspirado no livro *The Art of Computer Programming, Vol. 2*;
- O algoritmo de Rejeição do *Atkison*, inspirado no artigo *The Computer Generation of Poisson Random Variables, A. C. Atkison*

**Passo a Passo - Knuth**

**1.** Faça $L = e^{-\lambda}$, $k=0$ e $p=1$

**2.** Se $p \le L$, retorne $k-1$

**3.** Se não, faça $k = k+1$, gere $u \sim Uniforme(0,1)$ e $p = p \times u$ enquanto $p>L$

**Passo a Passo - Rejeição do Atkison**

Parâmetros utilizados:

- $\beta = \frac{\pi}{\sqrt{3\lambda}}$
- $\alpha = \beta\lambda$
-   $c<30 \rightarrow 0.767 - 3.36 / \lambda$
  $$c > 30 = \begin{cases}
  c>20 &\text{if } 0.5925 \\
  c.c &\text{if } 0.6506
  \end{cases}$$
- $k = log(c) - \lambda - log(\beta)$

**1.** Gere $U_1 \sim Uniforme(0,1)$ e faça $X = (\alpha - log{(1-U_1)/U_1})/\beta$

**2.** Se $X < - \frac{1}{2}$, volte para o Passo 1

**3.** Faça $N = [X+\frac{1}{2}]$, gere $U_2 \sim Uniforme(0,1)$

**4.** Se $\alpha - \beta X + log(U_2/\{1+exp(\alpha-\beta X)\}^2) \le k+Nlog\lambda-logN!$, retorne $N$, caso contrário volte para o Passo 1

```{r, fc_aux_poisson, echo=FALSE, warning=FALSE, message=FALSE}
require(microbenchmark)
require(ggplot2)
library(patchwork)

prob_plot <- function(lambda, amostra, freq_rel){
  plot(freq_rel, 
     main = paste("Lambda =", lambda), 
     xlab = "x", 
     ylab = "Frequência Relativa (Prob)")
  points(0:max(amostra), 
         dpois(0:max(amostra), lambda), 
         col = "red", 
         pch = 19,
         cex = 0.8)
}
```

```{r, poisson_algoritmos}
set.seed(233)
n <- 10000
lambda_p <- 5
lambda_g <- 150

# Livro Referência -----------------------------------------------
rpoisson_aux_livro <- function(lambda){
  u <- runif(1, 0, 1)
  i <- 0; pr <- exp(-lambda); Fx = pr
  while(u >= Fx){
    pr <- pr*lambda/(i+1)
    Fx <- Fx + pr
    i <- i+1
  }
  return(i)
}

rpoisson_livro <- function(n, lambda){
  replicate(n, expr = rpoisson_aux_livro(lambda), simplify = TRUE)
}

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

amostra_p <- rpoisson_livro(n, lambda_p)
freq_rel_p <- prop.table(table(amostra_p))
prob_plot(lambda_p, amostra_p, freq_rel_p)

amostra_g <- rpoisson_livro(n, lambda_g)
freq_rel_g <- prop.table(table(amostra_g))
prob_plot(lambda_g, amostra_g, freq_rel_g)

# Knuth ----------------------------------------------------------
rpoisson_aux_knuth <- function(lambda){
  L <- exp(-lambda); k <- 0; p <- 1
  
  while(p > L){
    k <- k+1
    u <- runif(1, 0, 1)
    p <- p*u
  }
  
  return(k-1)
}


rpoisson_knuth <- function(n, lambda){
  replicate(n, expr = rpoisson_aux_knuth(lambda), simplify = TRUE)
}

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

amostra_p <- rpoisson_knuth(n, lambda_p)
freq_rel_p <- prop.table(table(amostra_p))
prob_plot(lambda_p, amostra_p, freq_rel_p)


amostra_g <- rpoisson_knuth(n, lambda_g)
freq_rel_g <- prop.table(table(amostra_g))
prob_plot(lambda_g, amostra_g, freq_rel_g)


# Rejeição Atkison -------------------------------------------------------------------

rpoisson_aux_PA <- function(lambda) {
  if (lambda < 30) {
    if (lambda < 20) c <- 0.5925
    else c <- 0.6506
  } else {
    c <- 0.767 - 3.36 / lambda
  }
  
  beta <- pi / sqrt(3 * lambda)
  alpha <- beta * lambda
  k <- log(c) - lambda - log(beta)
  
  while (TRUE) {
    U1 <- runif(1)
    X <- (alpha - log((1 - U1) / U1)) / beta
    
    if (X >= -0.5) {
      N <- floor(X + 0.5)
      
      U2 <- runif(1)
      param <- alpha - beta * X + log(U2 / (1 + exp(alpha - beta * X))^2)
      log_factorial_N <- ifelse(N == 0, 0, lfactorial(N))
      
      poisson_form <- k + N * log(lambda) - log_factorial_N
      
      if (param <= poisson_form) {
        return(N)
      }
    }
  }
}

rpoisson_PA <- function(n, lambda){
  replicate(n, expr = rpoisson_aux_PA(lambda), simplify = TRUE)
}

par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

amostra_p <- rpoisson_PA(n, lambda_p)
freq_rel_p <- prop.table(table(amostra_p))
prob_plot(lambda_p, amostra_p, freq_rel_p)


amostra_g <- rpoisson_PA(n, lambda_g)
freq_rel_g <- prop.table(table(amostra_g))
prob_plot(lambda_g, amostra_g, freq_rel_g)


# Comparação ----------------------------------------------------------

comparacao <- function(n, lambda){
  mb_results <- microbenchmark(
    Livro_Referência = rpoisson_livro(n, lambda),
    Knuth = rpoisson_knuth(n, lambda),
    PA_Atkison = rpoisson_PA(n, lambda),
    Nativo_R = rpois(n, lambda),
    times = 100L 
  )
  return(mb_results)
}
```

```{r, poisson_tabelas, echo=FALSE, warning=FALSE, fig.width=10, fig.height=5}
# Tabela de resultados microbenchmark - Lambda Pequeno
mb_results_p <- comparacao(n, lambda_p)
print("------ Lambda Pequeno = 5 -----")
print(mb_results_p)

# Tabela de resultados microbenchmark - Lambda Grande
mb_results_g <- comparacao(n, lambda_g)
print("------ Lambda Grande = 150 -----")
print(mb_results_g)

plot1 <- autoplot(mb_results_p) +
  labs(title = paste("Desempenho para lambda =", lambda_p)) +
  theme_minimal()

plot2 <- autoplot(mb_results_g) +
  labs(title = paste("Desempenho para lambda =", lambda_g)) +
  theme_minimal()

plot1 + plot2
```

Através das comparações, podemos concluir que para lambdas pequenos, o método apresentado no livro referência é o melhor em questão de tempo com bastante folga, e o algoritmo da rejeição do Atkison teve o pior desempenho. Entretanto, para lambdas consideravelmente grandes, o algoritmo da rejeição do Atkison apresenta uma melhora de desempenho com uma diferença de média de $\approx 0.05$ segundos com o algoritmo apresentado no livro referência, sendo também um pouco mais consistente (ver gráfico de desempenho para $\lambda=150$).

## Exercício 1.7
Reescreva a função do Exemplo 2.6 permitindo que os possíveis valores de $X$ e as probabilidades sejam passadas como argumentos. Faça um estudo de simulação demonstrando a efetividade.

### Exemplo 2.6

Seja $x_1, \ldots, x_n$ uma amostra aleatória da distribuição Normal($\mu, \sigma^2$), $n \geq 2$, e $S^2$ a variância amostral. Então,

$$
\chi^2 = \frac{(n - 1)S^2}{\sigma^2} \sim \chi^2(n-1),
$$

Um intervalo de $100(1 - \alpha)\%$ de confiança unilateral para $\sigma^2$ é dado por

$$
IC_{100(1-\alpha)\%}(\sigma^2) = \left[0, \frac{(n - 1)s^2}{\chi^2_{\alpha}}\right],
$$

em que $\chi^2_{\alpha}$ é o quantil de ordem $\alpha$ da distribuição $\chi^2_{(n-1)}$.

Se a população amostrada é normal, então o nível de confiança é $1-\alpha$. Mas o que acontece se os dados não são normalmente distribuídos?

```{r}
# Dados com distribuicao normal com var=4
n <- 20; alpha <- .05
UCL <- replicate(1000, expr = {
  x <- rnorm(n, mean = 0, sd = 2)
  (n-1) * var(x) / qchisq(alpha, df = n-1) # ultimo calculo é retornado
} )
mean(UCL > 4)

# Se os dados têm distribuição qui-quadrado com variância igual a 4:

UCL <- replicate(1000, expr = {
  x <- rchisq(n, df = 2)
  (n-1) * var(x) / qchisq(alpha, df = n-1)
} )
mean(UCL > 4)
```

E como fica para uma t-student para diferentes valores do grau de liberdade, mas com variância igual a 4?

```{r}
n <- 30
m <- 10000
alpha <- .05
gl <- seq(3, 50, by = 2)
conf <- numeric( length(gl) )
for( l in 1:length(gl) ){
  UCL <- replicate(m, expr = {
    x <- rt(n, df = gl[l])*sqrt( (gl[l]-2)/gl[l] )*2
    (n-1) * var(x) / qchisq(alpha, df = n-1)
  } )
  conf[l] <- mean(UCL > 4)
}
par(mar = c(4,4,1,1))
plot(gl, conf, type = "b", pch = 16, cex = 0.75)
abline(h = 0.95, col = 2, lty = 2, lwd = 2)
```

E como fica para uma t-student para diferentes tamanhos amostrais?

```{r}
# Se os dados tem distribuicao t-student(0,sigma2,5) com var=4, mudando n
n <- seq(10, 600, by = 30)
m <- 10000
alpha <- .05; gl <- 5
conf <- numeric( length(n) )
for(l in 1:length(n)){
  UCL <- replicate(m, expr = {
    x <- rt(n[l], df = gl)*sqrt(4*(gl-2)/gl)
    (n[l]-1) * var(x) / qchisq(alpha, df = n[l]-1)
  }, simplify = T )
  conf[l] <- mean(UCL > 4)
}
par(mar = c(4,4,1,1))
plot(n, conf, type = "b", pch = 16, cex = 0.75, ylim = c(0.45, 1) )
abline(h = 0.95, col = 2, lty = 2, lwd = 2)
```

## Solução

Para esse exercício, implementaremos a função `rcategorica2`, que gera amostras de uma distribuição categórica arbitrária.

**Passo a Passo**

Parâmetros utilizados:

- Vetores de valores possíveis: $x = (x_1, x_2, ..., x_m)$
- Vetor de probabilidades: $p = (p_1, p_2, ..., p_m)$, com $\sum_{i=1}^{m} p_i = 1$

**1.** Valide as entradas: se os vetores não tiverem o mesmo tamanho, interrompa a execução; se o somatório das probabilidade é diferente de 1, normalize ($p_i \leftarrow p_i / \sum p_i$) e verifique se todas as probabilidades são maiores ou iguais a 0.

**2.** Ordene os valores e probabilidades em ordem decrescente de $p_i$ e calcule as probabilidades acumuladas.

**3.** Para cada amostra, gere $U_i \sim Uniforme(0,1)$ e encontre a primeira categoria $j$ tal que$U_i \le q_j$.

**4.** Defina $am[i] = x_{ordered}[j]$ e retorne o vetor de amostras $am = (am[1], am[2], ..., am[n])$.

```{r}
# Função para gerar amostras de uma distribuição categórica (rcategorica2 da fonte)
rcategorica2 <- function(n, x, p){
  # Validação de entradas
  if (length(x) != length(p)) stop("Os vetores de valores e probabilidades devem ter o mesmo comprimento.")
  if (sum(p) != 1) {
    warning("As probabilidades não somam 1. Elas serão normalizadas.")
    p <- p / sum(p)
  }
  if (any(p < 0)) stop("As probabilidades devem ser não negativas.")
  
  # Ordenar os valores e probabilidades em ordem decrescente de probabilidade
  ordem <- order(p, decreasing = TRUE)
  x_ordered <- x[ordem]
  p_ordered <- p[ordem]
  
  am <- numeric(n) # Vetor para armazenar as amostras
  acum <- cumsum(p_ordered) # Calcular as probabilidades acumuladas
  
  # Gerar n amostras
  for(i in 1:n){
    u <- runif(1) # Gerar um número aleatório uniforme
    # Encontrar a categoria correspondente
    for(j in 1:length(x_ordered)){
      if(u <= acum[j]){
        am[i] <- x_ordered[j]
        break # Sair do loop interno após encontrar a categoria
      }
    }
  }
  return(am)
}
```

Para verificar a efetividade por meio de simulação, temos os seguintes cenários:

```{r}
set.seed(123) # Para reprodutibilidade

# Número de amostras para cada cenário de simulação
n_samples_sim <- 50000

# Função auxiliar para plotar comparações (adaptada da ilustração da fonte)
plot_comparison <- function(amostra, theoretical_p, title_str, x_vals) {
  # Assegurar que os níveis do fator correspondam aos valores possíveis para ordenação correta
  observed_freq <- prop.table(table(factor(amostra, levels = x_vals)))
  
  # Encontrar o máximo valor para o limite do eixo Y
  max_y <- max(max(observed_freq), max(theoretical_p)) * 1.1
  
  # Plotar o histograma das frequências observadas
  barplot(observed_freq, ylab = "Frequência Relativa", xlab = "Valor de X",
          main = title_str, col = rainbow(length(x_vals), s = 0.7, v = 0.9), ylim = c(0, max_y))
  
  # Adicionar as probabilidades teóricas como pontos para comparação
  points(x = seq_along(x_vals), y = theoretical_p, col = "red", pch = 16, cex = 1.5)
  
  # Adicionar legenda
  legend("topright", legend = c("Frequência Observada", "Probabilidade Teórica"),
         col = c(NA, "red"), pch = c(NA, 16), lty = c(1, NA), bty = "n",
         fill = c(rainbow(length(x_vals), s = 0.7, v = 0.9), NA), border = c("black", NA))
}

# Cenário 1: Distribuição Uniforme Discreta (quatro categorias)
cat_x_1 <- 1:4
cat_p_1 <- c(0.25, 0.25, 0.25, 0.25)
sim_amostra_1 <- rcategorica2(n_samples_sim, cat_x_1, cat_p_1)

# Cenário 2: Distribuição Categórica Assimétrica (valores numéricos distintos)
cat_x_2 <- c(10, 20, 30, 40, 50)
cat_p_2 <- c(0.05, 0.15, 0.30, 0.25, 0.25)
sim_amostra_2 <- rcategorica2(n_samples_sim, cat_x_2, cat_p_2)

# Cenário 3: Distribuição Categórica com muitos níveis e valores não numéricos
cat_x_3 <- LETTERS[1:7] # Usando letras como possíveis valores
cat_p_3 <- c(0.05, 0.1, 0.15, 0.2, 0.15, 0.2, 0.15)
sim_amostra_3 <- rcategorica2(n_samples_sim, cat_x_3, cat_p_3)

# Plotar os resultados dos três cenários
par(mfrow = c(3, 1), mar = c(4, 4, 3, 1))

plot_comparison(sim_amostra_1, cat_p_1, "Cenário 1: Distribuição Uniforme Discreta", cat_x_1)
plot_comparison(sim_amostra_2, cat_p_2, "Cenário 2: Distribuição Categórica Assimétrica (Numérica)", cat_x_2)
plot_comparison(sim_amostra_3, cat_p_3, "Cenário 3: Distribuição Categórica com Caracteres", cat_x_3)
```

## Exercício 1.8 - MARI
Reimplemente o método da transformação inversa para gerar da distribuição Normal truncada sem utilizar as funções `pnorm` e `dnorm`. Utilize a função integrate para calcular a função de distribuição acumulada em um ponto. Use a função `qnorm`, visto que neste caso fica complicado implementar este método sem o uso desta função.

## Solução

**1.** Defina as funções `dnorm_manual` e `pnorm_manual` (que implementam a função densidade de probabilidade e a função distribuição acumulada da Normal, respectivamente).

**2.** Calcule as probabilidades acumuladas nos limites $a$ e $b$.

**3.** Gere números a partir de $U_i \sim \text{Uniforme}(0,1), \quad i = 1, \dots, n$

**4.** Ajuste as probabilidades para o truncamento: $P_i = \Phi_a + U_i \cdot (\Phi_b - \Phi_a)$

**5.** Obtenha as amostras da Normal truncada usando a função quantil `qnorm` $X_i = qnorm(P_i, \mu, \sigma)$

**6.** Defina a função da densidade teórica da Normal truncada para comparação gráfica $f_{\text{trunc}}(x) = \frac{f(x \mid \mu, \sigma^2)}{F(b) - F(a)}, \quad a \le x \le b$

```{r}
# 1. Função manual para a FDP da distribuição Normal (dnorm_manual)
# Esta função calcula phi(x | mu, sigma^2)
dnorm_manual <- function(x, mu, sigma) {
  if (sigma <= 0) stop("O desvio padrão (sigma) deve ser positivo.")
  
  exponent <- -((x - mu)^2 / (2 * sigma^2))
  coefficient <- 1 / (sigma * sqrt(2 * pi))
  return(coefficient * exp(exponent))
}

# 2. Função manual para a FDA da distribuição Normal (pnorm_manual)
# Esta função calcula Phi(q | mu, sigma^2) usando integrate
pnorm_manual <- function(q, mu, sigma) {
  if (sigma <= 0) stop("O desvio padrão (sigma) deve ser positivo.")
  
  # Casos extremos para q, onde a FDA é 0 ou 1
  if (q == -Inf) return(0)
  if (q == Inf) return(1)
  
  # Usar integrate para calcular a área sob a curva da FDP
  # desde -Inf até q.
  # A função integrate retorna uma lista, e o valor da integral está em 'value'.
  result <- tryCatch({
    integrate(f = function(x) dnorm_manual(x, mu, sigma),
              lower = -Inf, upper = q)
  }, error = function(e) {
    stop(paste("Erro durante a integração em pnorm_manual para q =", q, ":", e$message))
  })
  
  return(result$value)
}

# 3. Função reimplementada para gerar amostras da Normal Truncada
# Esta função calcula x = F_trunc_inv(u) usando pnorm_manual e qnorm
rnormtrunc_reimplemented <- function(n, mu, sigma, a, b) {
  # Validação de entradas
  if (sigma <= 0) stop("O desvio padrão (sigma) deve ser positivo.")
  if (a >= b) stop("O limite inferior 'a' deve ser menor que o limite superior 'b'.")
  
  # Gerar n números aleatórios uniformes entre 0 e 1
  us <- runif(n)
  
  # Calcular os valores da FDA não truncada nos pontos de truncamento 'a' e 'b'
  # Estes valores são constantes para todas as amostras e calculados uma vez.
  # Usamos max(0, min(1, ...)) para garantir que os valores estejam dentro de
  # para evitar erros com qnorm devido a pequenas imprecisões de integração.
  Phi_a <- max(0, min(1, pnorm_manual(a, mu, sigma)))
  Phi_b <- max(0, min(1, pnorm_manual(b, mu, sigma)))
  
  # Calcular P_u conforme a derivação matemática
  # Esta é a probabilidade acumulada ajustada que será usada em qnorm
  P_u <- Phi_a + us * (Phi_b - Phi_a)
  
  # Gerar as amostras da distribuição Normal truncada usando qnorm
  # qnorm(p, mean, sd) retorna o quantil para a probabilidade p
  amostra <- qnorm(P_u, mean = mu, sd = sigma)
  
  return(amostra)
}

# Função dnormtrunc original (do material fornecido) para comparação gráfica
# Esta função usa as funções dnorm e pnorm do R, que são permitidas para comparação
dnormtrunc <- function(x, mu, sigma, a, b){
  d <- dnorm(x, mu, sigma) / (pnorm(b, mu, sigma) - pnorm(a, mu, sigma))
  return(d)
}

### Exemplo de Uso e Ilustração

set.seed(123456789) # Para reprodutibilidade

# Parâmetros da distribuição Normal truncada
n_samples <- 10000
mu_val <- 10
sigma_val <- 2
a_val <- 6
b_val <- 13

# Gerar amostras usando a função reimplementada
amostra_reimplementada <- rnormtrunc_reimplemented(n_samples, mu_val, sigma_val, a_val, b_val)

# Configurações para o gráfico
par(mar = c(4, 4, 1, 1))

# Criar um histograma das amostras geradas
hist(amostra_reimplementada, xlab = "x", ylab = "Densidade", prob = TRUE, main = "",
     xlim = c(a_val, b_val), col = "lightblue", border = "darkblue",
     ylim = c(0, max(dnormtrunc(seq(a_val, b_val, length.out = 100), mu_val, sigma_val, a_val, b_val)) * 1.1))

# Adicionar a curva de densidade teórica para comparação
grid_x <- seq(a_val, b_val, length.out = 500)
lines(grid_x, dnormtrunc(grid_x, mu_val, sigma_val, a_val, b_val), col = "red", lwd = 2)

legend("topleft", legend = c("Amostras (Reimplementado)", "Densidade Teórica"),
       col = c("darkblue", "red"), lty = c(NA, 1), pch = c(22, NA), lwd = c(NA, 2),
       fill = c("lightblue", NA), border = c("darkblue", NA), bty = "n")
```

## Exercício 1.9
Implemente uma função para gerar amostras de tamanho $n$ da variável aleatória  com função densidade de probabilidade é dada por
$$
f(x) = \begin{cases}
  \frac{x-2}{2} &\text{se } 2 \le x \le 3\\
  \frac{2-x/3}{2} &\text{se } 3 \le x \le 6
  \end{cases}
$$
Apresente as contas necessárias.

## Solução 
A detalhar.

## Exercício 1.10
Implemente uma função para gerar amostras de tamanho $n$ da variável aleatória com função de distribuição é dada por
$F(x|\alpha, \beta) = 1 - exp\{-\alpha x^\beta\}, 0<x<\infty$
Apresente as contas necessárias.

##Solução
A função de distribuição dada é também conhecida como *Weibull* com função de densidade $f(x|\alpha,\beta) = (\alpha \beta x^{\beta-1})\times exp{-\alpha \beta}$ (encontrada através da derivada de $F(x|\alpha, \beta)$)

Seja $U \sim Uniforme(0,1)$, queremos encontrar $x$ tal que $\rightarrow U = 1 - exp\{-\alpha x^\beta\}$
    $$1-U = exp\{-\alpha x^\beta\}\\
    -ln(1-U) = \alpha x^\beta\\
    x^\beta = \frac{-ln(1-U)}{\alpha}\\
    x = \Bigg(\frac{-ln(1-U)}{\alpha}\Bigg)^\frac{1}{\beta}$$
  
Como $U \sim Uniforme(0,1) \rightarrow 1-U \sim Uniforme(0,1)$ e temos $x = \Bigg(\frac{-ln(U)}{\alpha}\Bigg)^\frac{1}{\beta}$

```{r weibull}
rweib <- function(n, alpha, beta){
  U <- runif(n)
  x <- (-log(U) / alpha)^(1/beta)
  return(x)
}

densidade_teorica <- function(x, alpha, beta) {
  alpha * beta * x^(beta - 1) * exp(-alpha * x^beta)
}

set.seed(123)  

alpha <- 2
beta <- 1.5
n <- 10000

amostras <- rweib(n, alpha, beta)

hist(amostras, breaks = 30, probability = TRUE, 
     main = "Histograma vs Densidade Teorica",
     xlab = "x", col = "lightblue")

x_seq <- seq(0, max(amostras), length.out = 1000)
lines(x_seq, densidade_teorica(x_seq, alpha, beta), col = "red", lwd = 2)

legend("topright", legend = c("Amostras", "Teorica"), 
       col = c("lightblue", "red"), lwd = c(2, 2))

```

## Exercício 1.11 - MARI
Escreva uma função para gerar variáveis aleatórias com distribuição Lognormal(1,0). Compare os métodos: método da transformação e método da composição. Gere amostras de tamanho 1000 e compare através de histogramas com a função densidade da distribuição lognormal dada pela função `dlnorm` do R.

## Solução

**Passo a Passo - Método da transformação**

**1.** Gere números a partir de $U_i \sim \text{Uniforme}(0,1), \quad i = 1, \dots, n$ - a função `rnormal_padrao`

**2.** Use o método de Box-Muller para converter os números uniformes em variáveis aleatórias que seguem uma distribuição normal padrão $N(0,1)$, calculando pares de valores dentro de um círculo unitário e aplicando uma transformação logarítmica e trigonométrica.

**3.** Aplique a transformação logarítmica à média e ao desvio padrão.

**4.** Exponencie a variável para obter a variável resultante lognormal.

```{r}
# Função para gerar variáveis aleatórias Normais padrão (N(0,1))
rnormal_padrao <- function(n){
  # Calcula o número de iterações necessárias para obter n amostras.
  # Cada iteração bem-sucedida gera 2 valores normais padrão.
  itera <- ceiling(n/2) 
  amostra <- numeric(itera * 2) # Pré-aloca espaço para a amostra
  
  for(i in 1:itera){
    s <- 2 # Inicializa s com um valor > 1 para entrar no loop while
    while(s > 1 || s == 0){ # A condição s == 0 é adicionada para robustez, embora rara
      u1 <- runif(1)
      u2 <- runif(1)
      v1 <- 2*u1-1 # Transforma U(0,1) para U(-1,1)
      v2 <- 2*u2-1 # Transforma U(0,1) para U(-1,1)
      s <- v1^2 + v2^2 # Calcula s = R^2
    }
    
    # Aplica as transformações de Box-Muller para obter X e Y ~ N(0,1)
    scale_factor <- sqrt(-2*log(s)/s)
    x <- scale_factor * v1
    y <- scale_factor * v2
    
    # Armazena os valores gerados
    amostra[i*2-1] <- x
    amostra[i*2] <- y
  }
  return(amostra[1:n]) # Retorna exatamente n amostras
}

# Função para gerar variáveis aleatórias Lognormais usando o método da transformação
rlnorm_transformacao <- function(n, meanlog = 1, sdlog = 1) {
  # Verifica se sdlog é positivo para uma distribuição não degenerada
  if (sdlog <= 0) {
    warning("sdlog deve ser positivo para uma distribuição Lognormal não degenerada. Retornando valores constantes.")
    return(rep(exp(meanlog), n))
  }
  
  # 1. Gerar variáveis Normais padrão (Z_std ~ N(0,1))
  z_std <- rnormal_padrao(n)
  
  # 2. Transformar para Normal com os parâmetros desejados (Z ~ N(meanlog, sdlog^2))
  # Se Z ~ N(0,1), então mu + sigma*Z ~ N(mu, sigma^2)
  z_general <- meanlog + sdlog * z_std
  
  # 3. Exponenciar para obter Lognormal (X = e^Z)
  x_lognormal <- exp(z_general)
  
  return(x_lognormal)
}
```

**Passo a Passo - Método da composição**

**1.** Escolha uma mistura de distribuições conhecidas.

**2.** Gere uma variável discreta para escolher a componente que será usada para cada amostra.

**3.** Gere a amostra da componente selecionada.

```{r}
gerar_lognormal_composicao <- function(n, pesos, params_meanlog, params_sdlog) {
  # Verifica se os parâmetros têm o mesmo comprimento
  if (length(pesos) != length(params_meanlog) || length(pesos) != length(params_sdlog)) {
    stop("Os vetores de pesos e parâmetros (meanlog, sdlog) devem ter o mesmo comprimento.")
  }
  
  num_componentes <- length(pesos)
  amostras_geradas <- numeric(n)
  
  for (i in 1:n) {
    # Passo 1: Seleciona qual componente amostrar com base nos pesos [2]
    # 'sample' retorna o índice da componente selecionada (1 a num_componentes)
    componente_selecionada <- sample(1:num_componentes, size = 1, prob = pesos)
    
    # Obtém os parâmetros da componente selecionada
    meanlog_atual <- params_meanlog[componente_selecionada]
    sdlog_atual <- params_sdlog[componente_selecionada]
    
    # Passo 2: Gera um valor aleatório da componente selecionada [2]
    amostras_geradas[i] <- rlnorm(1, meanlog = meanlog_atual, sdlog = sdlog_atual)
  }
  
  return(amostras_geradas)
}
```

Comparando com o `dlnorm`, temos:

```{r}
# Parâmetros da distribuição Lognormal
meanlog_param <- 1
sdlog_param <- 1
n_amostras <- 1000
pesos_mistura <- c(0.4, 0.6)
meanlog_componentes <- c(0, 2)
sdlog_componentes <- c(0.5, 0.8)

# 1. Gerar amostras usando o método da transformação
set.seed(123) # Para reprodutibilidade
amostra_transformacao <- rlnorm_transformacao(n_amostras, meanlog = meanlog_param, sdlog = sdlog_param)

# 2. Gerar as amostras usando o método da composição
amostra_lognormal_mistura <- gerar_lognormal_composicao(
  n_amostras,
  pesos_mistura,
  meanlog_componentes,
  sdlog_componentes
)

# 3. Gerar amostras usando a função dlnorm do R para comparação
amostra_r_dlnorm <- rlnorm(n_amostras, meanlog = meanlog_param, sdlog = sdlog_param)

# Definir o grid para a curva de densidade teórica
min_val <- min(amostra_transformacao, amostra_r_dlnorm)
max_val <- max(amostra_transformacao, amostra_r_dlnorm)
grid_x <- seq(min_val, max_val, length.out = 500)
densidade_teorica <- dlnorm(grid_x, meanlog = meanlog_param, sdlog = sdlog_param)

# Configurar o layout
par( mfrow = c(1,3), mar = c(4, 4, 3, 1) )

# Histograma da amostra gerada pelo método da transformação
hist(amostra_transformacao, freq = FALSE, breaks = 30,
     main = "Lognormal (Transformação)",
     xlab = "Valor", ylab = "Densidade",
     col = "lightblue", border = "darkblue",
     xlim = c(min_val, max_val), ylim = c(0, max(densidade_teorica) * 1.1))
lines(grid_x, densidade_teorica, col = "red", lwd = 2)
legend("topleft", legend = c("Densidade Teórica"), col = "red", lty = 1, lwd = 2, bty = "n")

# Histograma da amostra gerada pelo método da composição
hist(amostra_lognormal_mistura, prob = TRUE, breaks = 40,
     xlab = "x", ylab = "Densidade",
     main = "Distribuição Lognormal como Mistura (Método da Composição)",
     col = "lightgreen", border = "white", xlim = c(0, max(amostra_lognormal_mistura) * 1.1))

# Histograma da amostra gerada pela função rlnorm do R
hist(amostra_r_dlnorm, freq = FALSE, breaks = 30,
     main = "Lognormal (rlnorm do R)",
     xlab = "Valor", ylab = "Densidade",
     col = "lightgreen", border = "darkgreen",
     xlim = c(min_val, max_val), ylim = c(0, max(densidade_teorica) * 1.1))
lines(grid_x, densidade_teorica, col = "red", lwd = 2)
legend("topleft", legend = c("Densidade Teórica"), col = "red", lty = 1, lwd = 2, bty = "n")
```

## Exercício 1.12

## Exercício 1.17

## Exercício 1.18


 
