---
title: "Lista 2 Métodos Computacionais para Ciência de Dados II"
author: "Emanuelle Oliveira, Maria Luiza Oliveira, Mariana Fleming"
execute: 
  echo: true
  warning: false
  message: false
output:
  pdf_document:
    keep_tex: true
  html_document: default
---
# Questão 1
## Exercício 1.14
Pesquise algum pacote do R em que o método da rejeição adaptativa está implementado e gere amostras das distribuições Gama e Beta. Considere diferentes valores para os parâmetros destas distribuições.

## Solução
O pacote `ars` é feito para calcular o método da rejeição adaptativa, metódo este que gera variáveis aleatórias de distribuiçoes contínuas de probabilidade quando a função densidade-log é também a função log-concâvo. No pacote basta utilizar da função de mesmo nome.

### Gama(2, 1/10)

Priori do modelo autoregressivo não-Gaussiano baseado em uma skew-*t* de Juárez & Steel, 2010.

```{r ars-gamma, message=FALSE, warning=FALSE}
library(ars)

set.seed(830)
gamma <-function(x,shape,scale=1){
  (shape-1)*log(x)-x/scale
  }

gamma_dlog <- function(x,shape,scale=1) {
  (shape-1)/x-1/scale
}
n <- 100000
sample_gamma<-ars(n, gamma, gamma_dlog, x=4.5, m=1, lb=TRUE, xlb=0, shape=2, scale=0.1)

hist(sample_gamma,
     probability = TRUE,
     ylim = c(0, 4),
     main = "Distribuicao Gamma",
     col = 'lightblue')

x_seq <- seq(0, max(sample_gamma), length.out = 100000)
lines(x_seq, dgamma(x_seq, shape=2, scale=0.1), col = "tomato2", lwd = 3)
```

### Beta(3.4, 1.1)

```{r ars-beta, message=FALSE, warning=FALSE}
library(ars)

set.seed(830)
beta <- function(x,a,b){
  (a-1)*log(x)+(b-1)*log(1-x)
}

beta_dlog <-function(x,a,b){
  (a-1)/x-(b-1)/(1-x)
}

sample_beta <-ars(n, beta, beta_dlog, x=c(0.3,0.6), m=2, 
                  lb=TRUE, xlb=0, ub=TRUE, xub=1, 
                  a=3.4, b=1.1)

hist(sample_beta,
     probability = TRUE,
     ylim = c(0, 4),
     main = "Distribuicao Beta",
     col = 'lightblue')

x_seq <- seq(0, max(sample_beta), length.out = 100000)
lines(x_seq, dbeta(x_seq, 3.4, 1.1), col = "tomato2", lwd = 3)
```


## Execício 1.16
Implemente o algoritmo SIR para amostrar da distribuição normal assimétrica.

## Solução



## Execício 1.19
Considerando diferentes valores de $\lambda$, implemente o segundo algoritmo
para gerar do Processo de Poisson homogêneo e compare a eficiência computacional dos
dois algoritmos 

## Solução



## Exercício 1.21
Implemente um algoritmo eficiente para gerar os $k$ primeiros tempos de um Processo de Poisson não homogêneo
com função de intensidade

$$
\lambda(t) = \begin{cases}
  \frac{t}{5}, 0<t<5; \\
  1+5(t-5),  t\geq 5.
  \end{cases}
$$

## Solução



## Exercício 1.22
Implemente o algoritmo para gerar um processo de Poisson nãohomogêneo com função de intensidade dada no Exemplo 2.15. Faça com diferentes valores de $a$.

## Solução



## Exercício 1.23
Implemente um algoritmo para gerar um processo de Poisson homogêneo na área abaixo da curva $f(x)=x^2$ com $0\leq x \leq 5$.

## Solução


# Questão 2
Use o amostrador de Metropolis-Hastings para gerar variáveis aleatórias de uma distribuição Cauchy padrão. Descarte os primeiros 1000 da cadeia e compare os decis de observações geradas com os decis da distribuição Cauchy padrão (veja `qcauchy` ou `qt` com `df=1`). Lembre-se de que uma distribuição Cauchy($\theta, \eta$) tem função densidade

## Solução

### Passo a passo

1) Inicie uma cadeia onde o primeiro passo é um valor de entrada $X^{(0)}=x^{(0)}$, em que $X^{(t)}=(X^{(t)}_1, ..., X^{(t)}_p)^t$. Dado $X^{(0)}=x^{(0)}$, o algoritmo para gerar $X^{(t+1)}$ continua:

2) Escolha uma função proposta $Y \sim g(\cdot \vert x^{(t)})$. A escolhida para o exercício foi uma $N(x^{(t)}, \sigma^2)$.

3) Calculamos a razão $R = \frac{f(y)}{f(x^{(t)})}$, visto que $g(\cdot)$ se anula por ser uma normal

4) Aceite $x^{(t+1)}=Y$ com probabilidade $\min{(1, R(x^{(t))}, Y)}$, caso contrário, faça $x^{(t+1)} = x^{(t)}$

5) Incremente $t$ e retorne ao passo 2.

Calculamos a cauchy padrão na função `pdf_cauchy`, a mesma é definida como Cauchy(0,1). Utilizamos de entrada $x^{(0)} = 5$.
$$
\to f(x) = \frac{1}{\pi(1+x^2)}
$$

```{r cauchy, fig.width=10, fig.height=5}
pdf_cauchy <- function(x){
  result <- 1 / (pi * (1 + x^2))
  return (result)
}

mh_cauchy <- function(n, x0, sigma){
  chain <- numeric(n)
  chain[1] <- x0
  accepted = 0
  
  for (i in 2:length(chain)){
    x_current <- chain[i-1]
    
    # proposta: Normal com media de valor atual e sigma de entrada
    y <- rnorm(1, x_current, sigma)
    
    # R = f(Y) / f(x(t)) -> a g(.) se anula por ser uma normal
    ratio <- pdf_cauchy(y) / pdf_cauchy(x_current)
    
    if (runif(1) < ratio){
      chain[i] <- y 
      accepted<- accepted + 1
    } else {
      chain[i] <- x_current
    }
  }
  
  ARate <- accepted / (n-1)
  
  return(list(chain = chain, ARate = ARate))
}

n <- 10000
burn_in <- 1000

set.seed(123)  
result <- mh_cauchy(n, x0 = 5, sigma = 1)


chain <- result$chain
post_burn_chain <- chain[(burn_in + 1):n]

cat("Taxa de aceitacao:", round(result$ARate, 4), "\n")

plot(post_burn_chain, type = "l", col = "cornflowerblue", 
     main = "Cadeia (pos burn-in)",
     xlab = "Iteracao", ylab = "Valor")

hist(post_burn_chain, breaks = 50, freq = FALSE, 
     main = "Histograma vs Densidade Teorica", 
     xlab = "x", ylab = "Densidade", col = "lightblue") 
curve(dcauchy(x, location = 0, scale = 1), add = TRUE, 
      col = "tomato2", lwd = 2, lty = 2)

```



# Questão 3
Implemente um amostrador Metropolis de passeio aleatório para gerar a distribuição Laplace padrão. Para o incremento, simule de uma distribuição normal. Compare as cadeias geradas quando diferentes variâncias são usadas para a distribuição de proposta. Além disso, compute as taxas de aceitação de cada cadeia.

## Solução

### Passo a passo

1) Inicie uma cadeia onde o primeiro passo é um valor de entrada $x^{(0)}=\mu^{(0)}$, onde iremos utilizar a mesma $\mu$ para todas as cadeias e $\sigma^2$ um valor escolhido para cada cadeia. Dado $X^{(0)}=x^{(0)}$, o algoritmo para gerar $X^{(t+1)}$ continua:

2) Escolha uma função proposta $Y \sim g(\cdot \vert x^{(t)})$. A escolhida para o exercício foi uma $N(\mu^{(t)}, \sigma^{2*})$. Utilizamos no exemplo $\mu^{(0)} = x^{(0)}=5, \sigma^2=0.2, 2, 5$ e $10$.

3) Calculamos a razão $R = \frac{f(y)}{f(x^{(t)})}$, visto que $g(\cdot)$ se anula por ser uma normal

4) Aceite $x^{(t+1)}=Y$ com probabilidade $\min{(1, R(x^{(t)}, Y))}$, caso contrário, faça $x^{(t+1)} = x^{(t)}$

5) Incremente $t$ e retorne ao passo 2.

Calculamos a cauchy padrão na função `pdf_laplace`, a mesma é definida como Laplace(0,1).
$$
f(x) = \frac{1}{2b}exp{\Big(-\frac{\vert x-\mu\vert}{b} \Big)} \\
\to f(x) = \frac{1}{2}exp{(-\vert x \vert)}
$$

### Código

```{r laplace, fig.width=10, fig.height=5}
pdf_laplace <- function(x){
  result <- (1 / 2)*exp(- abs(x))
  return (result)
}

mh_laplace <- function(n, x0, sigma){
  chain <- numeric(n)
  chain[1] <- x0
  accepted = 0
  
  for (i in 2:length(chain)){
    x_current <- chain[i-1]
    
    # proposta: Normal com media de valor atual e sigma de entrada
    y <- rnorm(1, x_current, sigma)
    
    # R = f(Y) / f(x(t)) -> a g(.) se anula por ser uma normal
    ratio <- pdf_laplace(y) / pdf_laplace(x_current)
    
    if (runif(1) < ratio){
      chain[i] <- y 
      accepted<- accepted + 1
    } else {
      chain[i] <- x_current
    }
  }
  
  ARate <- accepted / (n-1)
  
  return(list(chain = chain, ARate = ARate))
}

n <- 10000
burn_in <- 1000

set.seed(123)  

result1 <- mh_laplace(n, x0 = 5, sigma = 0.2)
result2 <- mh_laplace(n, x0 = 5, sigma = 2)
result3 <- mh_laplace(n, x0 = 5, sigma = 5)
result4 <- mh_laplace(n, x0 = 5, sigma = 10)

ARate1 <- round(result1$ARate, 4)
ARate2 <- round(result2$ARate, 4)
ARate3 <- round(result3$ARate, 4)
ARate4 <- round(result4$ARate, 4)

taxas_acept <- data.frame(
  Sigma = c("0.2", "2", "5", "10"),
  TaxaAceitacao  = c(ARate1, ARate2, ARate3, ARate4)
)
print(taxas_acept)
```

### Gráficos de cadeia

```{r chains-laplace, echo=FALSE, fig.width=10, fig.height=5}
plot_cadeia <- function(result, sigma_val, burn_in) {
  post_burn_chain <- result$chain[(burn_in + 1):n]
  
  plot(post_burn_chain, type = "l", col = "cornflowerblue", 
       main = paste("Sigma =", sigma_val),
       xlab = "Iteracao", ylab = "Valor")
}

plot_histograma <- function(result, sigma_val, burn_in) {
  post_burn_chain <- result$chain[(burn_in + 1):n]
  
  hist(post_burn_chain, breaks = 50, freq = FALSE, 
       main = paste("Sigma =", sigma_val), 
       xlab = "x", ylab = "Densidade", col = "lightblue")
  
  curve(pdf_laplace(x), add = TRUE, 
        col = "tomato2", lwd = 2, lty = 2)
}

par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
plot_cadeia(result1, 0.2, burn_in)
plot_cadeia(result2, 1, burn_in)
plot_cadeia(result3, 5, burn_in)
plot_cadeia(result4, 10, burn_in)
```

### Histograma vs Densidade Teórica

```{r hist_laplace, echo=FALSE, fig.width=10, fig.height=5}
par(mfrow = c(1, 1))

par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
plot_histograma(result1, 0.2, burn_in)
plot_histograma(result2, 1, burn_in)
plot_histograma(result3, 5, burn_in)
plot_histograma(result4, 10, burn_in)
```


# Questão 4
Considere a densidade bivariada
$$
f(x,y) \propto \dbinom{n}{k} y^{x+a-1} (1-y)^{n-x+b-1}, x=0,1,...,n, 0 \leq y \leq 1.
$$

Pode-se mostrar que, para $a, b, n$ fixos, as distribuições condicionais são Binomial$(n, y)$ e Beta$(x + a, n - x + b)$. Use o amostrador de Gibbs para gerar uma cadeia com densidade conjunta alvo $f(x, y)$.

## Solução
A densidade marginal é:
$$
f(x) = \int_0^1 \dbinom{n}{k} y^{x+a-1} (1-y)^{n-x+b-1}dy
$$
Que é a Beta-Binomial da nossa função densidade alvo.

Entretanto, como essa integral pode ser complexa, iremos usar Gibbs para gerar de $f(x \vert y) \sim Bin(n, y)$ e $f(y \vert x) \sim Beta(x+a, n-x+b)$.

### Passo a passo

1) Defina os valores iniciais de $x^{(0)}$ e $y^{(0)}$. Nesse exemplo usaremos $a=2, b=4$ e $t=16$.

2) Enquanto $i \leq N, i=1,2...$, gere:
$$
x^{(i)} \sim Binomial(t, y^{(i-1)}) \\
y^{(i)} \sim Beta(a+x^{(i)}, t-x^{(i)}+b)
$$
```{r gibbs, message=FALSE, warning=FALSE}
library(VGAM)

pdf_betabinomial <- function(x, size=16, a=2, b=4) {
  dbetabinom.ab(x, size=size, shape1=a, shape2=b)
}

gs_betabinomial <- function(N, a, b, t, x0, y0){
  X <- numeric(n)
  Y <- numeric(n)
  
  X[1] <- x0
  Y[1] <- y0
  
  for(i in 2:N){
    X[i] <- rbinom(1, t, Y[i-1])
    Y[i] <- rbeta(1, X[i]+a, t-X[i]+b)
  }
  
  return(list(X = X, Y = Y))
}

set.seed(469)

N <- 5000
burn_in <- 1000

result <- gs_betabinomial(N, a=2, b=4, t=16, x0=2, y0=0.5)

result_X <- result$X
result_Y <- result$Y
```

```{r chains-gibbs, echo=FALSE,fig.width=10, fig.height=5}
plot_cadeia <- function(result, var, burn_in) {
  post_burn_chain <- result[(burn_in + 1):N]
  
  plot(post_burn_chain, type = "l", col = "cornflowerblue", 
       main = paste("Cadeia de ", var),
       xlab = "Iteracao", ylab = "Valor")
}

plot_cadeia(result_X, "X", burn_in)
plot_cadeia(result_Y, "Y", burn_in)
```

### Histograma vs Densidade Teórica

```{r hist_gibbs, echo=FALSE, fig.width=10, fig.height=5}
x_vals <- 0:20
teorica <- pdf_betabinomial(x_vals, size = 16, a = 2, b = 4)

hist(result_X[(burn_in + 1):N], breaks = seq(-0.5, 16.5, by = 1), 
     freq = FALSE, col = "lightblue",
     main = "Distribuicao Marginal de X vs Densidade Teorica Conjunta",
     xlab = "x", ylab = "Densidade",
     ylim = c(0, max(teorica) * 1.1))
lines(x_vals, teorica, type = "l", col = "tomato2", lwd = 2)

```

# Questão 5
Suponha que a densidade conjunta de $X, Y, Z$ seja dada por

$$
f(x,y,z) = C e^{-(x+y+z+axy+bxz+cyz)}, \quad x > 0,\; y > 0,\; z > 0,
$$

onde $a, b, c$ são constantes não negativas especificadas e $C$ não depende de $x, y, z$. Explique como podemos simular o vetor$(X, Y, Z)$ e realize uma simulação para estimar $E[X]$ e $E[XYZ]$ quando $a = b = c = 1$.

## Solução
Utilizaremos o método da aceitação e rejeição, uma forma adequada de gerar amostras de distribuições contínuas quando a função de densidade de probabilidade alvo pode ser calculada, mesmo sem a constante de normalização, e não é trivial amostrar diretamente dela. Esse método permite amostrar de uma distribuição alvo $f(x)$ usando uma distribuição proposta $g(x)$ da qual é fácil amostrar, desde que a densidade alvo seja "envelopada" pela proposta multiplicada por uma constante.

### Definições iniciais

1) Defina a densidade alvo (não normalizada): uma vez que a constante é desconhecida, podemos trabalhar com a função de densidade alvo não normalizada ($q(x,y,z) = e^{-(x+y+z+axy+bxz+cyz)}$).

2) Escolha uma densidade proposta $g(x,y,z)$: nesse caso, uma escolha natural é uma distribuição exponencial independente para cada variável (tomemos $\lambda=1$) para cada componente, porque é fácil amostrar desse densidade e tem o mesmo suporte da densidade alvo (pois $(x>0, y>0, z>0)$). Temos então $g(x,y,z) = e^{-x} e^{-y} e^{-z} = e^{-(x+y+z)}$ para $x,y,z > 0$.

3) Determine a constante $c$ do envelope: a constante $c$ é definida como o valor máximo de $q(x,y,z)/g(x,y,z)$ sobre todo o domínio das variáveis.

$$ \frac{q(x,y,z)}{g(x,y,z)} = \frac{e^{-(x+y+z+axy+bxz+cyz)}}{e^{-(x+y+z)}} = e^{-(axy+bxz+cyz)} $$
Como $a, b, c$ são constantes não negativas e $x, y, z > 0$, o termo $axy+bxz+cyz$ é sempre não negativo e por consequência, $-(axy+bxz+cyz)$ é sempre não positivo. O valor máximo de $e^{-(axy+bxz+cyz)}$ ocorre quando $axy+bxz+cyz$ é mínimo, ou seja, quando $x,y,z$ se aproximam de zero. Nesse limite, $e^{-(axy+bxz+cyz)} \to e^0 = 1$. Dessa forma, podemos escolher $\mathbf{c = 1}$.

### Passo a passo

Repita os passos abaixo até que uma amostra seja aceita:

1) Gere um vetor candidato $(Y_x, Y_y, Y_z)$ da distribuição proposta $g(x,y,z)$ (o que significa gerar $Y_x \sim \text{Exponencial}(1)$, $Y_y \sim \text{Exponencial}(1)$ e $Y_z \sim \text{Exponencial}(1)$ independentemente).

2) Gere um número aleatório $U$ de uma distribuição Uniforme(0,1).

3)Calcule a razão de aceitação ($r = e^{-(aY_xY_y+bY_xY_z+cY_yY_z)}$).

4)Se $U < r$ razão, aceite o candidato. Ele será o vetor $(X, Y, Z)$ amostrado. Caso contrário, rejeite o candidato e retorne ao passo 1.

A eficiência deste algoritmo é a probabilidade média de aceitação, que é $\frac{1}{c} \int q(y)dy / \int g(y)dy$. Como $g(y)$ é uma densidade de probabilidade (portanto $\int g(y)dy = 1$) e $c=1$, a eficiência é $\int q(y)dy$. Este valor é o inverso da constante de normalização $C$ da fdp alvo (ou seja, $\text{Eficiência} = 1/C$). Um valor de $1/C$ próximo de 1 indica alta eficiência.

Dada uma sequência de $N$ amostras independentes $(X_j, Y_j, Z_j)$ da distribuição desejada, podemos estimar a esperança de qualquer função $g(X,Y,Z)$ como a média amostral de $g(X_j,Y_j,Z_j)$.


```{r simulacao-vetores}
gerar_vetor <- function(a, b, c) {
  accepted <- FALSE
  while (!accepted) {
    Yx <- rexp(1, rate = 1)
    Yy <- rexp(1, rate = 1)
    Yz <- rexp(1, rate = 1)
    
    U <- runif(1)
    razao <- exp(-(a * Yx * Yy + b * Yx * Yz + c * Yy * Yz))

      if (U < razao) {
      accepted <- TRUE
      return(c(Yx, Yy, Yz))
    }
  }
}

# parâmetros
a <- 1
b <- 1
c <- 1
n <- 10000

matriz_simulacao <- matrix(NA, nrow = n, ncol = 3)
tentativas <- 0

# simulação
for (i in 1:n) {
  tentativa_atual <- 0
  aceito <- FALSE
  while (!aceito) { #aq estava accepted_once
    tentativa_atual <- tentativa_atual + 1
    Yx_cand <- rexp(1, rate = 1)
    Yy_cand <- rexp(1, rate = 1)
    Yz_cand <- rexp(1, rate = 1)
    U_cand <- runif(1)
    val_razao <- exp(-(a * Yx_cand * Yy_cand + b * Yx_cand * Yz_cand + c * Yy_cand * Yz_cand))

    if (U_cand < val_razao) {
      matriz_simulacao[i, ] <- c(Yx_cand, Yy_cand, Yz_cand)
      aceito <- TRUE
    }
  }
  tentativas <- tentativas + tentativa_atual
}

amostraX <- matriz_simulacao[, 1]
amostraY <- matriz_simulacao[, 2]
amostraZ <- matriz_simulacao[, 3]

esperancaX <- mean(amostraX)
XYZ <- amostraX * amostraY * amostraZ
esperancaXYZ <- mean(XYZ)

erropX <- sd(amostraX)/sqrt(n)
erropXYZ <- sd(XYZ)/sqrt(n)

eficiencia <- n/tentativas

library(knitr)
resultados <- data.frame(
  Medida = c("Número de amostras simuladas",
             "Eficiência do método de Aceitação-Rejeição",
             "Estimativa de E[X]",
             "Erro Padrão de E[X]",
             "Estimativa de E[XYZ]",
             "Erro Padrão de E[XYZ]"),
  Valor = c(n,
            round(eficiencia, 4),
            round(esperancaX, 4),
            round(erropX, 4),
            round(esperancaXYZ, 4),
            round(erropXYZ, 4))
)

kable(resultados, caption = "Simulação Monte Carlo (a=b=c=1)")
```

# Questão 6

## Exercício 2.1
Utilizando o método de integração de Monte Carlo, obtenha uma aproximação para as seguintes integrais.

a) $\int_{0}^{1} \exp\{ e^x \}\, dx$ 

b) $\int_{-2}^{2} \exp\{ x + x^2 \}\, dx$  

c) $\int_{0}^{\infty} x (1 + x^2)^{-2}\, dx$

f) $\int_{0}^{\infty} \int_{0}^{x} \exp\{ -(x+y) \}\, dy\, dx$

### Soluções

a) 
$$
I \approx \frac{1}{N}\sum^N_{i=1}f(U_i)(b-a) \\
\to \frac{1}{N}\sum^N_{i=1}e^{e^{x_i}}(1-0)
$$

```{r integracao-a}
set.seed(345)

f <- function(x){
  exp(exp(x))
}

n <- 10000
x <- runif(n)
aprox <- mean(f(x))

numerica <- integrate(f = function(x){exp(exp(x))}, lower=0, upper=1) 

resultado <- data.frame(
  Metodo = c("Monte Carlo", "Integracao numerica"),
  Valor  = c(aprox, numerica$value)
)
print(resultado)

```

b)
$$
I \approx \frac{1}{N}\sum^N_{i=1}f(U_i)(b-a) \\
\to \frac{1}{N}\sum^N_{i=1}e^{x+x^2}(2-(-2))
$$

```{r integracao-b}
set.seed(345)

f <- function(x){
  exp(x-x^2)
}

n <- 10000
x <- runif(n, -2, 2)
aprox <- mean(f(x))* (2 - (-2)) #multiplicamos pela largura

numerica <- integrate(f = function(x){exp(x-x^2)}, lower=-2, upper=2)

resultado <- data.frame(
  Metodo = c("Monte Carlo", "Integracao numerica"),
  Valor  = c(aprox, numerica$value)
)
print(resultado)
```

c)
$$
x = \frac{u}{1-u}, u \in (0,1) \text{ em } x \in (0, \infty) \\
\frac{dx}{du} = \frac{1}{(1-u)^2} \\
\int_0^1f(x(u))\frac{dx}{du}du = E_U[g(U)] \\
\to I = \frac{1}{N}\sum^N_{i=1}g(U_i)
$$

```{r integracao-c}
set.seed(345)

g <- function(u){
  (u*(1-u))/(2*u^2 - 2*u + 1)^2
}

n <- 10000
u <- runif(n)
aprox <- mean(g(u))

numerica <- integrate(f = function(x){x*(1+x^2)^(-2)}, lower=0, upper=Inf)

resultado <- data.frame(
  Metodo = c("Monte Carlo", "Integracao numerica"),
  Valor  = c(aprox, numerica$value)
)
print(resultado)
```

f) Nesse caso, temos $P(Y \leq X)$ quando $X$ e $Y$ são iid $Exp(1)$.

```{r integracao-f, message=FALSE, warning=FALSE}
set.seed(345)
library(pracma)

f <- function(x, y) {
  exp(-(x + y))
}

n <- 10000
x <- rexp(n, rate = 1)
y <- rexp(n, rate = 1)

aprox <- mean(y <= x)

numerica <- integral2(f, 0, 100, 0, function(x){x}) #n grande para simulação, visto que o pracma não aceita Inf

resultado <- data.frame(
  Metodo = c("Monte Carlo", "Integracao numerica"),
  Valor  = c(aprox, numerica$value)
)
print(resultado)
```

## Exercício 2.2
Faça um estudo Monte Carlo para estimar o viés e o EQM dos seguintes estimadores para a variância populacional:

$$
\hat{\sigma}_1^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 
\quad \text{e} \quad 
\hat{\sigma}_2^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2.
$$

Gere os dados de diferentes distribuições e considere diferentes tamanhos amostrais. Utilize $m = 1000$ réplicas de Monte Carlo. Estime o erro padrão destas estimativas.

### Solução

Para cada réplica, vamos gerar uma amostra e calcular os dois estimadores da variância (o não viciado e o viciado), guardando-os em vetores. Depois, vamos retornar as medidas (viés, EQM e seus erros padrão) para cada estimador.

```{r estimadores-variancia}
# estruturação
library(knitr)
set.seed(2023032088)
m <- 1000
n <- c(10, 30, 100)
resultados <- list()

simulacao <- function(n, m, dist, mu, sigma2, r_function) {
  sigma2_real <- sigma2
  nao_viciado <- numeric(m)
  viciado <- numeric(m)
  
  for (j in 1:m) {
    x <- r_function(n)
    # cálculo dos estimadores
    nao_viciado[j] <- var(x) 
    viciado[j] <- (n - 1) / n * nao_viciado[j]
  }
  
  vies <- function(estimador) {
    G_vies <- estimador - sigma2_real 
    vies_estimado <- mean(G_vies)
    ep_vies <- sd(G_vies) / sqrt(m)
    return(c(Vies = vies_estimado, SE_Vies = ep_vies))
  }
  
  eqm <- function(estimador) {
    G_eqm <- (estimador - sigma2_real)^2 
    eqm_estimado <- mean(G_eqm)
    ep_eqm <- sd(G_eqm) / sqrt(m)
    return(c(EQM = eqm_estimado, SE_EQM = ep_eqm))
  }

  vies1 <- vies(nao_viciado)
  eqm1  <- eqm(nao_viciado)
  vies2 <- vies(viciado)
  eqm2  <- eqm(viciado)
  
  res1 <- data.frame(
    Distribuicao = dist, n = n, Estimador = "Sigma1^2 (Não Viciado)",
    vies = vies1["Vies"], se_vies = vies1["SE_Vies"],
    eqm = eqm1["EQM"], se_eqm = eqm1["SE_EQM"]
  )
  res2 <- data.frame(
    Distribuicao = dist, n = n, Estimador = "Sigma2^2 (Viciado)",
    vies = vies2["Vies"], se_vies = vies2["SE_Vies"],
    eqm = eqm2["EQM"], se_eqm = eqm2["SE_EQM"]
  )
  
  return(rbind(res1, res2))
}

# exemplos
# N(10, 4), sigma^2 = 4
mu_norm <- 10
sigma2_norm <- 4
r_norm <- function(n) { rnorm(n, mean = mu_norm, sd = sqrt(sigma2_norm)) }

for (n in n) {
  resultados[[length(resultados) + 1]] <- simulacao(
    n = n, m = m, dist = "Normal (N(10, 4))", mu = mu_norm, 
    sigma2 = sigma2_norm, r_function = r_norm
  )
}

# Exp(0.5), sigma^2 = 1/0.5^2 = 4
lambda_exp <- 0.5
mu_exp <- 1/lambda_exp
sigma2_exp <- 1/lambda_exp^2
r_exp <- function(n) { rexp(n, rate = lambda_exp) }

for (n in n) {
  resultados[[length(resultados) + 1]] <- simulacao(
    n = n, m = m, dist = "Exponencial (Exp(0.5))", mu = mu_exp, 
    sigma2 = sigma2_exp, r_function = r_exp
  )
}


tabela_final <- do.call(rbind, resultados)
tabela_final[, 4:7] <- round(tabela_final[, 4:7], 6)
kable(tabela_final, caption = "Viés e EQM da Variância")
```


## Exercício 2.4
Suponha o intervalo de 95% de confiança t-Student para a média populacional com dados não normais. A probabilidade de que o intervalo de confiança cubra a verdadeira média não é necessariamente 0,95, pois uma suposição não é atendida. Use um estudo Monte Carlo para estimar a probabilidade de cobertura (e o erro padrão desta estimativa) do intervalo t-Student para amostras de tamanhos $n$=20, 30 e 100 das seguintes distribuições:

a) $\chi^2_2$

b) Outra distribuição assimétrica de sua preferência.

### Passo a passo

1) A partir de uma função auxiliar (`estimar`), calcule a cobertura empírica como a proporção de vezes que o IC continha o valor verdadeiro da média e o erro padrão da cobertura pela fórmula $\sqrt{p(1-p)/m}$.

2) Para cada n, gera m amostras de tamanho n da distribuição.

3) Aplique o teste t em cada amostra para construir o IC de 95%.

4) Verifique se a média verdadeira está dentro do intervalo e armazene os resultados.

### Solução

```{r pre-ic}
set.seed(2023032088)
m <- 20000
alpha <- 0.05
confianca_nominal <- 1 - alpha
n <- c(20, 30, 100)
resultados <- list()
i <- 1

estimar <- function(coberturas, m, dist, n) {
  y_chapeu <- mean(coberturas)
  ep_estimado <- sqrt(y_chapeu * (1 - y_chapeu) / m)
  
  return(data.frame(
    Distribuicao = dist,
    n = n,
    Cobertura_Empirica = y_chapeu,
    Erro_Padrao = ep_estimado)
    )
}
```

a)

```{r ic-quiquadrado}
mu_chisq <- 2

for (n in n) {
  coberturas_chisq <- replicate(m, expr = {
    x <- rchisq(n, df = 2)
    teste_t <- t.test(x, conf.level = confianca_nominal)
    confianca <- teste_t$conf.int
    confianca[1] <= mu_chisq && mu_chisq <= confianca[2]
    }
  )

  resultados[[i]] <- estimar(coberturas_chisq, m, "Chi^2_2 (mu=2)", n)
  i <- i + 1
}
```

b) A distribuição escolhida foi uma $Gama(2,1)$.

```{r ic-gama}
mu_gamma <- 2 

for (n in n) {
  coberturas_gamma <- replicate(m, expr = {
    x <- rgamma(n, shape = 2, rate = 1)
    teste_t <- t.test(x, conf.level = confianca_nominal)
    confianca <- teste_t$conf.int
    confianca[1] <= mu_gamma && mu_gamma <= confianca[2]
    }
  )
  resultados[[i]] <- estimar(coberturas_gamma, m, "Gamma(2, 1) (mu=2)", n)
  i <- i + 1
}
```

Resultados:

```{r resultados2.4}
tabela_final <- do.call(rbind, resultados)
tabela_final$Cobertura_Empirica <- round(tabela_final$Cobertura_Empirica, 4)
tabela_final$Erro_Padrao <- round(tabela_final$Erro_Padrao, 5)
kable(tabela_final, caption = "IC de 95% para distribuições assimétricas")
```

## Exercício 2.5
Plote a curva de poder empírica para o teste $t$ no Exemplo 3.7, mudando as hipóteses para $H_0 : \mu = 500$ vs $H_1 : \mu \ne 500$ e mantendo o nível de significância de $\alpha = 0,05$. Em um mesmo gráfico, plote as curvas de poder para os tamanhos amostrais 10, 20, 30, 40 e 50, usando cores diferentes.

### Exemplo 3.7

Suponha que $X_1, \dots, X_n$ seguem distribuição $N(\mu, \sigma^2)$. Deseja-se testar, com nível de significância $\alpha = 0,05$:

$$
H_0: \mu = 500 \quad \text{vs} \quad H_1: \mu > 500
$$

Sob a hipótese nula,  

$$
T = \frac{\bar{X} - 500}{S/\sqrt{n}} \sim t_{n-1}.
$$  

onde $t_{n-1}$ denota a distribuição $t$-Student com $n-1$ graus de liberdade.

a) Use o método de Monte Carlo para calcular a probabilidade do erro tipo I quando $\sigma = 100$;

b) Use simulação para estimar o poder e plotar a curva de poder empírico.

```{r}
## Estimando o nível de significância
n <- 20
alpha <- .05
mu0 <- 500
sigma <- 100
m <- 10000 # número de réplicas

p <- numeric(m) # guardando pvalor
for (j in 1:m) {
  x <- rnorm(n, mu0, sigma)
  ttest <- t.test(x, alternative = "greater", mu = mu0)
  p[j] <- ttest$p.value
}

p.hat <- mean(p < alpha)
se.hat <- sqrt(p.hat * (1 - p.hat) / m)
print(c(p.hat, se.hat))

## Avaliando o erro tipo I para diferentes valores de mu para o teste mu<=500
mu0s <- seq(450, 500, by=1)
p <- numeric(m) #storage for p-values
p.hat <- numeric(length(mu0s))
for(l in 1:length(mu0s)){
  for (j in 1:m) {
    x <- rnorm(n, mu0s[l], sigma)
    ttest <- t.test(x, alternative = "greater", mu = mu0)
    p[j] <- ttest$p.value
  }
  p.hat[l] <- mean(p < alpha)
}

plot(mu0s, p.hat, type="b", pch=16, cex=0.5, ylim=c(0,0.06))
abline(h=0.05, col=2, lty=2)

##  Funcao poder
n <- 20; m <- 1000; mu0 <- 500; sigma <- 100
mu <- c(seq(450, 650, 10)) 
k <- length(mu); power <- numeric(k)
for (i in 1:k) {
  mu_real <- mu[i]
  pvalues <- replicate(m, expr = {
    x <- rnorm(n, mean = mu_real, sd = sigma)
    ttest <- t.test(x, alternative = "greater", mu = mu0)
    ttest$p.value } )
  power[i] <- mean(pvalues < .05)
}

plot(mu, power, xlab = bquote(theta))
abline(v = mu0, lty = 2, col=2); abline(h = .05, lty = 2,col=4)

# adicionando erro padrão
library(Hmisc)
se <- sqrt(power * (1-power) / m)
errbar(mu, power, yplus = power+se, yminus = power-se, add = T)
lines(mu, power, lty=3)
detach(package:Hmisc)
```

### Passo a passo

1) Crie uma matriz para guardar as probabilidades de rejeição de $H_0$ quando falsa (o poder), com linhas representando diferentes valores de $\mu$ alternativos e colunas representando diferentes tamanhos amostrais.

2) Para cada tamanho de amostra n, repita o processo de simulação.

3) Para cada valor de média $\mu$ da sequência, simule m amostras da normal, aplique o teste t e calcule a proporção de rejeições (essa proporção é armazenada na matriz como a estimativa do poder).

### Solução

```{r curva-poder}
set.seed(2023032088)
m <- 5000
alpha <- 0.05
mu0 <- 500
sigma <- 100
n <- c(10, 20, 30, 40, 50)

mu_alternativa <- seq(400, 600, by = 5)

# matriz para armazenar as estimativas de poder (linhas = mu, colunas = n)
poder <- matrix(NA, nrow = length(mu_alternativa), ncol = length(n))
colnames(poder) <- paste0("n=", n)

for (j in 1:length(n)) {
  novo_n <- n[j]
  
  for (i in 1:length(mu_alternativa)) {
    mu_real <- mu_alternativa[i]
    
    pvalores <- replicate(m, expr = {
      x <- rnorm(novo_n, mean = mu_real, sd = sigma) 
      teste_t <- t.test(x, alternative = "two.sided", mu = mu0)
      teste_t$p.value
      }
    )
    poder[i, j] <- mean(pvalores < alpha)
  }
}

cores <- c("blue", "red", "green4", "purple", "orange")
curvas <- paste("n =", n)
par(mar = c(5, 4, 4, 6))

plot(mu_alternativa, poder[, 1], 
     type = "l", 
     col = cores[5], 
     lwd = 2,
     ylim = c(0, 1),
     xlab = expression(paste("Média Populacional (", mu, ")")), 
     ylab = "Poder",
     main = "Curva de Poder Empírica para o Teste t Bilateral",
     las = 1)

for (j in 2:length(n)) {
  lines(mu_alternativa, poder[, j], col = cores[j], lwd = 2)
}

# referência
abline(h = alpha, lty = 2, col = "gray50")
abline(v = mu0, lty = 3, col = "gray50")

legend("topright",
       inset = c(-0.3,0),
       legend = curvas,
       col = cores, 
       lwd = 2,
       bty = "n",
       xpd = TRUE)

kable(round(poder[c(1, 11, 21, 31, 41), ], 4), caption = "Poder estimado")
```

## Exercício 2.7
Implemente um algoritmo para fazer um teste de hipóteses Monte Carlo para testar se a média populacional é igual a um valor pré-especificado, assumindo que os dados são normalmente distribuidos com desvio padrão conhecido. Em um pequeno estudo Monte Carlo, compare o poder do teste implementado com o teste Normal (utilize a função implementada em algum pacote do R).

### Passo a passo

1) Calcule a estatística de teste na amostra observada e simule várias novas amostras como se a hipótese nula fosse verdadeira.

2) Compare os valores simulados com o valor observado e obtenha o p-valor pela frequência relativa.

3) Para cada repetição, gere uma amostra com média 11,5 e aplique tanto o método de Monte Carlo quanto o teste Z analítico.

4) Registre se cada método rejeitou ou não a hipótese nula.

5) O poder de cada método é medido pela proporção de vezes em que rejeitaram a hipótese nula. Calcule também o erro padrão dessas estimativas, para saber a precisão do resultado.

### Solução

```{r th-media}
testez <- function(x, mu0, sigma, m = 10000) {
  n <- length(x)
  ep_mu <- sigma / sqrt(n)
  z_obs <- (mean(x) - mu0) / ep_mu
  xbarra_estrela <- rnorm(m, mean = mu0, sd = ep_mu)
  
  # estatística para cada média simulada
  z_estrela <- (xbarra_estrela - mu0) / ep_mu
  pvalor_mc <- sum(abs(z_estrela) >= abs(z_obs)) / m
  
  return(list(
    z_obs = z_obs,
    pvalor_mc = pvalor_mc
  ))
}

library(TeachingDemos) # função z.test
n <- 30
sigma <- 5
mu0 <- 10
mu1 <- 11.5
alpha <- 0.05
m <- 10000

rejeicoes_mc <- numeric(m)
rejeicoes_norm <- numeric(m)

set.seed(2023032088)
for (i in 1:m) {
  x_amostra <- rnorm(n, mean = mu1, sd = sigma)
  
  resultado_mc <- testez(x_amostra, mu0 = mu0, sigma = sigma, m = m)
  if (resultado_mc$pvalor_mc < alpha) {
    rejeicoes_mc[i] <- 1
  }

  resultado_norm <- z.test(x_amostra, mu = mu0, stdev = sigma,
                           alternative = "two.sided")
  if (resultado_norm$p.value < alpha) {
    rejeicoes_norm[i] <- 1
  }
}

poder_mc <- mean(rejeicoes_mc)
poder_norm <- mean(rejeicoes_norm)

ep_poder_mc <- sqrt(poder_mc * (1 - poder_mc) / m)
ep_poder_norm <- sqrt(poder_norm * (1 - poder_norm) / m)

tabela <- data.frame(
  Teste = c("THMC (Monte Carlo)", "Normal Analítico (Z-Test)"),
  Poder_Estimado = c(poder_mc, poder_norm),
  Erro_Padrao_MC = c(ep_poder_mc, ep_poder_norm)
)

kable(tabela, caption = "Comparação do Poder de Teste (mu real = 11.5)")
```