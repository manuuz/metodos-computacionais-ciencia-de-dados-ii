---
title: "Lista 3 Métodos Computacionais para Ciência de Dados II"
author: "Emanuelle Oliveira, Maria Luiza Oliveira, Mariana Fleming"
execute: 
  echo: true
  warning: false
  message: false
output:
  pdf_document:
    keep_tex: true
  html_document: default
---
```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(kableExtra)
library(ggplot2)
```


# Questão 1

Considere $\theta=\int_{0}^{1}e^{x}dx$. Use uma simulação de Monte Carlo para estimar pelo método da variável antitética e também pelo método de Monte Carlo simples. Calcule uma estimativa empírica da redução percentual na variância ao usar a variável antitética.

## Solução

### Passo a passo

1. Defina a função integrando $g(x) = e^x$, cuja integral em $[0,1]$ é $e - 1$.

2. Simule $U_i \sim \text{Uniforme}(0,1)$. Esses números aleatórios serão usados para aproximar a esperança de $g(U)$.

3. Primeiro, no método de Monte Carlo simples, calcule $X_i = e^{U_i}$. A média de $X_i$ é uma estimativa de $\theta$ e a variância do estimador é $\text{Var}_{X_i}/k$.

4. Já no método da variável antitética, use os pares $(U_i, 1-U_i)$ (negativamente correlacionados) e calcule a média $Y_i = \frac{e^{U_i} + e^{1-U_i}}{2}$. A média de $Y_i$ é uma estimativa alternativa com menor variância.

5. Calcule o erro padrão para comparar os dois métodos e demais métricas que julgar relevante (aqui usamos a redução percentual da variância através da fórmula $\text{Redução} = \frac{\text{Var}_{MC} - \text{Var}_{ANT}}{\text{Var}_{MC}} \times 100$).

### Implementação do código

```{r}
# parâmetros
set.seed(123456789)
k <- 100000
g <- function(x) exp(x)

# Monte Carlo simples
U_mc <- runif(k)
X_mc <- g(U_mc)
tc_mc <- mean(X_mc) # tc = theta chapéu
var_tc_mc <- var(X_mc) / k
ep_tc_mc <- sqrt(var_tc_mc)

# variável antitética
U_ant <- 1 - U_mc
Y_ant <- (g(U_mc) + g(U_ant)) / 2
tc_ant <- mean(Y_ant)
var_tc_ant <- var(Y_ant) / k
ep_tc_ant <- sqrt(var_tc_ant)

reducao <- ((var_tc_mc - var_tc_ant) / var_tc_mc) * 100 # em percentual
valor_exato <- exp(1) - 1

resultado <- data.frame(
  Método = c("Monte Carlo", "Antitética"),
  Estimativa = c(tc_mc, tc_ant),
  Variância = c(var_tc_mc, var_tc_ant),
  ErroPadrão = c(ep_tc_mc, ep_tc_ant)
)

kable(resultado)
cat("\nValor exato:", round(valor_exato, 7),
    "\nRedução percentual na variância:", round(reducao, 4), "%\n")
```

Portanto, percebemos que a técnica da variável antitética melhora a eficiência da estimativa, uma vez que reduz a variância.

# Questão 2

Suponha que Y seja uma variável aleatória normal com média 1 e variância 1, e que, condicionalmente a $Y=y$, a variável X seja normal com média y e variância 4. Deseja-se usar simulação para estimar eficientemente $\theta=P\{X>1\}$.

**a) Explique como obter o estimador de simulação de Monte Carlo simples.**

## Solução

**b) Mostre como a esperança condicional pode ser usada para obter um estimador aprimorado.**

## Solução

```{r}

```

**c) Mostre como o estimador do item (b) pode ser ainda mais aprimorado utilizando Y como variável de controle.**

## Solução

```{r}

```

**d) Implemente os três métodos acima e mostre a variabilidade dos estimadores replicando o uso de cada algoritmo 200 vezes.**

## Solução

```{r}

```

# Questão 3

Obtenha uma estimativa de Monte Carlo para $\int_{1}^{\infty}\frac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}dx$ utilizando o método de amostragem por importância (importance sampling).

## Solução

No \textit{Importance Sampling} podemos reescrever a integral alvo ($f(x)$) de uma forma que possamos amostrar de uma distribuição de probabilidade proposta ($g(x)$) da nossa escolha, para isso reescrevemos:

$\int f(x)dx = \int g(x) \frac{f(x)}{g(x)}dx = \int g(x)w(x)dx$ onde $w(x)$ chamamos de peso.

Utilizaremos dessa integral como valor esperado da função $w(x) = \frac{f(x)}{g(x)}$ quando amostrarmos $X$ da proposta $g(x)$:

$I = E_g[w(x)]$

Logo, nosso Monte Carlo para $I$ com $N$ amostras será a média:

$Î = \frac{1}{N}\sum_{i=1}^{N} w(x)$

Temos como alvo $f(x) = \int_{1}^{\infty}\frac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}dx$, que pode ser interpretada como $f(x) = x^2 \cdot \phi(x)$, onde $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}$ é a densidade da Normal Padrão.
Como proposta escolhemos a Normal Truncada no intervalo da integral alvo, $[1, \infty)$, $g(x)=\frac{\phi(x)}{1-\Phi(1)}$, onde o denominador iremos chamar de constante normalizadora $C=1-\Phi(1)$.

Reconstruindo nossa função de peso $w(x)$ teremos:

$w(x) = \frac{f(x)}{g(x)} = \frac{x^2 \cdot \phi(x)}{\frac{\phi(x)}{C}} = C \cdot x^2$

Então teremos nosso estimador de Monte Carlo:

$Î = \frac{1}{N}\sum_{i=1}^{N}C\cdot x_i^2$

Amostra proposta $g(x)$:
```{r amostra-proposta-sir}
dnormtrunc <- function(x, mu=0, sigma=1, a, b){
  d <- dnorm(x, mu, sigma)/( pnorm(b, mu, sigma) - pnorm(a, mu, sigma) )
}

rnormtrunc <- function(n, mu=0, sigma=1, a, b){
  us <- runif(n)
  amostra <- qnorm( pnorm(a, mu, sigma) + 
                      ( pnorm(b, mu, sigma) - pnorm(a, mu, sigma) )*us, mu, sigma )
}

a=1
b=1000000 #Aproximacao de um numero muito grande como "infinito"
amostra_proposta <- rnormtrunc(100000, a=a, b=b)

par(mar = c(4, 4, 1, 1))
hist(amostra_proposta, col = "lightblue", xlab = expression(x), ylab = "Densidade", 
     prob = T, main = "")
grid <- seq(a, b, by = 0.01)
lines(grid, dnormtrunc(grid, a=a, b=b), col = "red", lwd = 2 )

```

Simulação de Monte Carlo:
```{r monte-carlo-sir}
N <- 100000
C <- 1 - pnorm(1)
amostras_x <- rnormtrunc(n = N, mu = 0, sigma = 1, a = 1, b = Inf)

estimativa_I <- mean(C * amostras_x^2)
valor_exato <- dnorm(1)

tabela <- data.frame(
  Metodo = c("Estimativa de Monte Carlo", "Valor Analitico"),
  Valor = c(estimativa_I, valor_exato)
)

kable(tabela, 
      caption = "Metodo para Integral",
      booktabs = T) %>%
    kable_styling(bootstrap_options = c("simple", "hover"), 
                  full_width = F,
                  position = "center")
``` 

# Questão 4

Utilize o algoritmo de Newton-Raphson para estimar os parâmetros de um modelo de regressão Poisson em que $y_{i}\sim Poisson(\lambda=exp\{\beta_{0}+\beta_{1}x_{i}\})$. Gere dados do modelo de regressão e avalie se o algoritmo fornece boas estimativas.

## Solução

```{r}

```

# Questão 5

Implemente um estudo Monte Carlo com $M=100$ réplicas para avaliar o algoritmo EM implementado nos slides para o modelo Poisson inflado de zeros e avalie as estimativas nos 4 cenários apresentados nos slides.

## Solução

```{r}

```

# Questão 6

Implemente o algoritmo EM para mistura de duas distribuições Normais apresentado nos slides e avalie a estimação em 4 cenários alterando tamanho amostral e valores do parâmetros.

## Solução
Considere uma amostra aleatória \(X_1, \ldots, X_n\) com função densidade de probabilidade dada por:

\[
f(x_i|\theta) = \sum_{k=1}^{K} P(S_i = k) P(X_i = x_i | S_i = k)
= \sum_{k=1}^{K} p_k \, \mathcal{N}(x_i|\mu_k, \sigma_k^2),
\]

onde \(\mathcal{N}(x_i|\mu_k, \sigma_k^2)\) é a densidade normal com média \(\mu_k\) e variância \(\sigma_k^2\), e

\[
\theta = (p_1, \ldots, p_K, \mu_1, \ldots, \mu_K, \sigma_1^2, \ldots, \sigma_K^2).
\]

Em particular, para \(K=2\):

\[
f(x_i|\theta) = p\,\mathcal{N}(x_i|\mu_1, \sigma_1^2) + (1-p)\,\mathcal{N}(x_i|\mu_2, \sigma_2^2).
\]

A função de verossimilhança para dados completos é:

\[
L(\theta|x,s) = \prod_{i=1}^{n}
[p\,\mathcal{N}(x_i|\mu_1,\sigma_1^2)]^{I\{s_i=1\}}
[(1-p)\,\mathcal{N}(x_i|\mu_2,\sigma_2^2)]^{I\{s_i=2\}}.
\]

A função de log-verossimilhança correspondente é:

\[
\begin{aligned}
l(\theta|x,s) &=
\sum_{i=1}^{n} I\{s_i=1\} \left[\ln p - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_1^2 - \frac{(x_i - \mu_1)^2}{2\sigma_1^2}\right] \\
&\quad + \sum_{i=1}^{n} I\{s_i=2\} \left[\ln (1-p) - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_2^2 - \frac{(x_i - \mu_2)^2}{2\sigma_2^2}\right].
\end{aligned}
\]


No passo E, define-se:

\[
\gamma_{i1}^{(t)} = E[I\{S_i=1\} | x_i, \theta^{(t)}] = P(S_i=1|x_i,\theta^{(t)}),
\]
\[
\gamma_{i2}^{(t)} = E[I\{S_i=2\} | x_i, \theta^{(t)}] = P(S_i=2|x_i,\theta^{(t)}).
\]

Assim,

\[
\begin{aligned}
Q(\theta|\theta^{(t)}) &=
\sum_{i=1}^{n} \gamma_{i1}^{(t)} \left[\ln p - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_1^2 - \frac{(x_i - \mu_1)^2}{2\sigma_1^2}\right] \\
&\quad + \gamma_{i2}^{(t)} \left[\ln (1-p) - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_2^2 - \frac{(x_i - \mu_2)^2}{2\sigma_2^2}\right].
\end{aligned}
\]

No passo M, atualizamos os parâmetros maximizando \(Q(\theta | \theta^{(t)})\):

\[
p^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n}\gamma_{i1}^{(t)}, \quad
1 - p^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n}\gamma_{i2}^{(t)}.
\]

De forma análoga:

\[
\mu_1^{(t+1)} = \frac{\sum_i \gamma_{i1}^{(t)} x_i}{\sum_i \gamma_{i1}^{(t)}}, \quad
\mu_2^{(t+1)} = \frac{\sum_i \gamma_{i2}^{(t)} x_i}{\sum_i \gamma_{i2}^{(t)}},
\]

\[
(\sigma_1^2)^{(t+1)} = \frac{\sum_i \gamma_{i1}^{(t)} (x_i - \mu_1^{(t+1)})^2}{\sum_i \gamma_{i1}^{(t)}}, \quad
(\sigma_2^2)^{(t+1)} = \frac{\sum_i \gamma_{i2}^{(t)} (x_i - \mu_2^{(t+1)})^2}{\sum_i \gamma_{i2}^{(t)}}.
\]

Passo a Passo do Algoritmo:

1. Inicialize \(p^{(0)}, \mu_1^{(0)}, \mu_2^{(0)}, (\sigma_1^2)^{(0)}, (\sigma_2^2)^{(0)}\);
2. Faça \(t = 0\);
3. Repita até convergência:
   - **Passo E:** Calcule \(\gamma_{i1}^{(t)}\) e \(\gamma_{i2}^{(t)}\);
   - **Passo M:** Atualize \(p^{(t+1)}, \mu_1^{(t+1)}, \mu_2^{(t+1)}, (\sigma_1^2)^{(t+1)}, (\sigma_2^2)^{(t+1)}\);
   - Faça \(t \leftarrow t + 1\).


```{r mistura-normais}
mistura_em <- function(x, mu_init, sigma_init, p_init, max_iter = 100, tol = 1e-6) {
  n <- length(x)
  mu <- mu_init     
  sigma <- sigma_init
  p <- p_init
  
  log_veross <- numeric(max_iter)
  
  for (t in 1:max_iter){
    dcomp1 <- p * dnorm(x, mean = mu[1], sd = sigma[1])
    dcomp2 <- (1 - p) * dnorm(x, mean = mu[2], sd = sigma[2])
    
    dtotal <- dcomp1 + dcomp2
  
    
    gamma1 <- dcomp1 / dtotal 
    gamma2 <- dcomp2 / dtotal
    
    log_veross[t] <- sum(log(dtotal))
    
    n1 <- sum(gamma1)
    n2 <- sum(gamma2)
    
    p_cand <- n1 / n
    
    mu1_cand <- sum(gamma1 * x) / n1
    mu2_cand <- sum(gamma2 * x) / n2
    
    sigma1_sq_cand <- sum(gamma1 * (x - mu1_cand)^2) / n1
    sigma2_sq_cand <- sum(gamma2 * (x - mu2_cand)^2) / n2

    if (sigma1_sq_cand <= 0 || sigma2_sq_cand <= 0) {
      break
    }
    
    p <- p_cand
    mu <- c(mu1_cand, mu2_cand)
    sigma <- c(sqrt(sigma1_sq_cand), sqrt(sigma2_sq_cand))
    
    if (t > 1 && abs(log_veross[t] - log_veross[t-1]) < tol) {
      log_veross <- log_veross[1:t] 
      break
    }
  }
  
  return(list(
    p = p,
    mu = mu,
    sigma = sigma,
    log_veross = log_veross,
    iter = t,
    gammas = data.frame(gamma1 = gamma1, gamma2 = gamma2)
  ))
}
```

### Cenários:
```{r mistura-normais-construcao-cenarios, message=FALSE, warning=FALSE}
gerar_mistura <- function(n, p_verd, mu_verd, sigma_verd) {
  componente <- sample(1:2, size = n, replace = TRUE, prob = c(p_verd, 1 - p_verd))
  x <- ifelse(componente == 1, 
              rnorm(n, mean = mu_verd[1], sd = sigma_verd[1]),
              rnorm(n, mean = mu_verd[2], sd = sigma_verd[2]))
  return(x)
}

avaliar_cenario <- function(nome_cenario, n, p_verd, mu_verd, sigma_verd) {
  set.seed(123)
  
  x <- gerar_mistura(n, p_verd, mu_verd, sigma_verd)

  mu_init <- quantile(x, c(0.25, 0.75)) 
  sigma_init <- c(sd(x), sd(x))         
  p_init <- 0.5                          
  
  resultado_em <- mistura_em(x, mu_init, sigma_init, p_init)

  ordem_verd <- order(mu_verd)
  mu_verd <- mu_verd[ordem_verd]
  sigma_verd <- sigma_verd[ordem_verd]
  p_verd <- if (ordem_verd[1] == 1) p_verd else (1 - p_verd)

  ordem_est <- order(resultado_em$mu)
  mu_est <- resultado_em$mu[ordem_est]
  sigma_est <- resultado_em$sigma[ordem_est]
  p_est <- if (ordem_est[1] == 1) resultado_em$p else (1 - resultado_em$p)

  tabela_resultados <- data.frame(
    Parametros = c("p", "Mu", "Sigma"),
    Verdadeiro = c(
      sprintf("%.3f", p_verd),
      sprintf("(%.3f, %.3f)", mu_verd[1], mu_verd[2]),
      sprintf("(%.3f, %.3f)", sigma_verd[1], sigma_verd[2])
    ),
    Estimado_EM = c(
      sprintf("%.3f", p_est),
      sprintf("(%.3f, %.3f)", mu_est[1], mu_est[2]),
      sprintf("(%.3f, %.3f)", sigma_est[1], sigma_est[2])
    )
  )
  
  tabela_formatada <- kable(tabela_resultados,
                            booktabs = T) %>%
    kable_styling(bootstrap_options = c("simple", "hover"), 
                  full_width = F,
                  position = "center")
 
  densidade_mistura <- function(x, p, mu, sigma) {
    p * dnorm(x, mu[1], sigma[1]) + (1 - p) * dnorm(x, mu[2], sigma[2])
  }
  
  df_plot <- data.frame(x = x)
  
  p <- ggplot(df_plot, aes(x = x)) +
    geom_histogram(aes(y = ..density..), bins = 50, fill = "lightblue", 
                   color = "black", alpha = 0.7) +
    stat_function(fun = densidade_mistura, args = list(p = p_verd, mu = mu_verd, 
                                                       sigma = sigma_verd),
                  aes(color = "Verdadeira"), size = 1.2) +
    stat_function(fun = densidade_mistura, args = list(p = p_est, mu = mu_est, 
                                                       sigma = sigma_est),
                  aes(color = "Estimada"), size = 1.2, linetype = "dashed") +
    scale_color_manual(name = "Densidade", 
                       values = c("Verdadeira" = "red3", "Estimada" = "dodgerblue4")) +
    labs(title = nome_cenario,
         subtitle = paste("n =", n),
         x = "Valor",
         y = "Densidade") +
    theme_minimal()
  
  print(p)
  
  cat(paste("O algoritmo convergiu em", resultado_em$iter, "iterações"))
  
  return(tabela_formatada)
}
```

**Cenário 1**

- $n = 2000$

- $p = 0.5$

- $\mu = (0, 5)$

- $\sigma = (1, 1)$

```{r mistura-normais-cenario1, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Grande, Bem Separado",
  n = 2000,
  p_verd = 0.5,
  mu_verd = c(0, 5),
  sigma_verd = c(1, 1)
)
```


**Cenário 2**

- $n = 2000$

- $p = 0.5$

- $\mu = (0, 2)$

- $\sigma = (1, 0.8)$

```{r mistura-normais-cenario2, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Grande, Muito Próximo",
  n = 2000,
  p_verd = 0.5,
  mu_verd = c(0, 2),
  sigma_verd = c(1, 0.8)
)
```

**Cenário 3**

- $n = 200$

- $p = 0.5$

- $\mu = (0, 6)$

- $\sigma = (1, 1)$

```{r mistura-normais-cenario3, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Pequena, Bem Separado",
  n = 200,
  p_verd = 0.5,
  mu_verd = c(0, 5),
  sigma_verd = c(1, 1)
)
```


**Cenário 4**

- $n = 200$

- $p = 0.5$

- $\mu = (0, 2)$

- $\sigma = (1, 0.8)$

```{r mistura-normais-cenario4, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Pequena, Muito Próximo",
  n = 200,
  p_verd = 0.5,
  mu_verd = c(0, 2),
  sigma_verd = c(1, 0.8)
)
```

**Cenário 5**

- $n = 700$

- $p = 0.3$

- $\mu = (0, 5)$

- $\sigma = (1, 1)$

```{r mistura-normais-cenario5, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Bem separado, p Deslabanceado",
  n = 700,
  p_verd = 0.3,
  mu_verd = c(0, 5),
  sigma_verd = c(1, 1)
)
```

**Cenário 6**

- $n = 700$

- $p = 0.3$

- $\mu = (0, 2)$

- $\sigma = (1, 0.8)$

```{r mistura-normais-cenario6, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Muito Próximo, p Deslabanceado",
  n = 700,
  p_verd = 0.3,
  mu_verd = c(0, 2),
  sigma_verd = c(1, 0.8)
)
```

# Questão 7

Implemente o algoritmo ECM para mistura de duas distribuições Gamas apresentado nos slides de variantes do algoritmo EM. Avalie as estimativas gerando dados de alguns cenários com mistura de duas gamas.

## Solução

```{r}

```