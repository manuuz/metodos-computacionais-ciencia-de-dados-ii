---
title: "Lista 3 Métodos Computacionais para Ciência de Dados II"
author: "Emanuelle Oliveira, Maria Luiza Oliveira, Mariana Fleming"
execute: 
  echo: true
  warning: false
  message: false
output:
  pdf_document:
    keep_tex: true
  html_document: default
---
```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(kableExtra)
library(ggplot2)
library(stats)
```


# Questão 1

Considere $\theta=\int_{0}^{1}e^{x}dx$. Use uma simulação de Monte Carlo para estimar pelo método da variável antitética e também pelo método de Monte Carlo simples. Calcule uma estimativa empírica da redução percentual na variância ao usar a variável antitética.

## Solução

### Passo a passo

1. Defina a função integrando $g(x) = e^x$, cuja integral em $[0,1]$ é $e - 1$.

2. Simule $U_i \sim \text{Uniforme}(0,1)$. Esses números aleatórios serão usados para aproximar a esperança de $g(U)$.

3. Primeiro, no método de Monte Carlo simples, calcule $X_i = e^{U_i}$. A média de $X_i$ é uma estimativa de $\theta$ e a variância do estimador é $\text{Var}_{X_i}/k$.

4. Já no método da variável antitética, use os pares $(U_i, 1-U_i)$ (negativamente correlacionados) e calcule a média $Y_i = \frac{e^{U_i} + e^{1-U_i}}{2}$. A média de $Y_i$ é uma estimativa alternativa com menor variância.

5. Calcule o erro padrão para comparar os dois métodos e demais métricas que julgar relevante (aqui usamos a redução percentual da variância através da fórmula $\text{Redução} = \frac{\text{Var}_{MC} - \text{Var}_{ANT}}{\text{Var}_{MC}} \times 100$).

### Implementação do código

```{r ex1}
# parâmetros
set.seed(123456789)
k <- 100000
g <- function(x) exp(x)

# Monte Carlo simples
U_mc <- runif(k)
X_mc <- g(U_mc)
tc_mc <- mean(X_mc) # tc = theta chapéu
var_tc_mc <- var(X_mc) / k
ep_tc_mc <- sqrt(var_tc_mc)

# variável antitética
U_ant <- 1 - U_mc
Y_ant <- (g(U_mc) + g(U_ant)) / 2
tc_ant <- mean(Y_ant)
var_tc_ant <- var(Y_ant) / k
ep_tc_ant <- sqrt(var_tc_ant)

reducao <- ((var_tc_mc - var_tc_ant) / var_tc_mc) * 100 # em percentual
valor_exato <- exp(1) - 1

resultado <- data.frame(
  Método = c("Monte Carlo", "Antitética"),
  Estimativa = c(tc_mc, tc_ant),
  Variância = c(var_tc_mc, var_tc_ant),
  ErroPadrão = c(ep_tc_mc, ep_tc_ant)
)

kable(resultado)
cat("\nValor exato:", round(valor_exato, 7),
    "\nRedução percentual na variância:", round(reducao, 4), "%\n")
```

Portanto, percebemos que a técnica da variável antitética melhora a eficiência da estimativa, uma vez que reduz a variância.

# Questão 2

Suponha que Y seja uma variável aleatória normal com média 1 e variância 1, e que, condicionalmente a $Y=y$, a variável X seja normal com média y e variância 4. Deseja-se usar simulação para estimar eficientemente $\theta=P\{X>1\}$.

**a) Explique como obter o estimador de simulação de Monte Carlo simples.**

## Solução

**b) Mostre como a esperança condicional pode ser usada para obter um estimador aprimorado.**

## Solução

```{r}

```

**c) Mostre como o estimador do item (b) pode ser ainda mais aprimorado utilizando Y como variável de controle.**

## Solução

```{r}

```

**d) Implemente os três métodos acima e mostre a variabilidade dos estimadores replicando o uso de cada algoritmo 200 vezes.**

## Solução

```{r}

```

# Questão 3

Obtenha uma estimativa de Monte Carlo para $\int_{1}^{\infty}\frac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}dx$ utilizando o método de amostragem por importância (importance sampling).

## Solução

No \textit{Importance Sampling} podemos reescrever a integral alvo ($f(x)$) de uma forma que possamos amostrar de uma distribuição de probabilidade proposta ($g(x)$) da nossa escolha, para isso reescrevemos:

$\int f(x)dx = \int g(x) \frac{f(x)}{g(x)}dx = \int g(x)w(x)dx$ onde $w(x)$ chamamos de peso.

Utilizaremos dessa integral como valor esperado da função $w(x) = \frac{f(x)}{g(x)}$ quando amostrarmos $X$ da proposta $g(x)$:

$I = E_g[w(x)]$

Logo, nosso Monte Carlo para $I$ com $N$ amostras será a média:

$Î = \frac{1}{N}\sum_{i=1}^{N} w(x)$

Temos como alvo $f(x) = \int_{1}^{\infty}\frac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}dx$, que pode ser interpretada como $f(x) = x^2 \cdot \phi(x)$, onde $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}$ é a densidade da Normal Padrão.
Como proposta escolhemos a Normal Truncada no intervalo da integral alvo, $[1, \infty)$, $g(x)=\frac{\phi(x)}{1-\Phi(1)}$, onde o denominador iremos chamar de constante normalizadora $C=1-\Phi(1)$.

Reconstruindo nossa função de peso $w(x)$ teremos:

$w(x) = \frac{f(x)}{g(x)} = \frac{x^2 \cdot \phi(x)}{\frac{\phi(x)}{C}} = C \cdot x^2$

Então teremos nosso estimador de Monte Carlo:

$Î = \frac{1}{N}\sum_{i=1}^{N}C\cdot x_i^2$

Amostra proposta $g(x)$:
```{r amostra-proposta-sir}
dnormtrunc <- function(x, mu=0, sigma=1, a, b){
  d <- dnorm(x, mu, sigma)/( pnorm(b, mu, sigma) - pnorm(a, mu, sigma) )
}

rnormtrunc <- function(n, mu=0, sigma=1, a, b){
  us <- runif(n)
  amostra <- qnorm( pnorm(a, mu, sigma) + 
                      ( pnorm(b, mu, sigma) - pnorm(a, mu, sigma) )*us, mu, sigma )
}

a=1
b=1000000 #Aproximacao de um numero muito grande como "infinito"
amostra_proposta <- rnormtrunc(100000, a=a, b=b)

par(mar = c(4, 4, 1, 1))
hist(amostra_proposta, col = "lightblue", xlab = expression(x), ylab = "Densidade", 
     prob = T, main = "")
grid <- seq(a, b, by = 0.01)
lines(grid, dnormtrunc(grid, a=a, b=b), col = "red", lwd = 2 )

```

Simulação de Monte Carlo:
```{r monte-carlo-sir}
N <- 100000
C <- 1 - pnorm(1)
amostras_x <- rnormtrunc(n = N, mu = 0, sigma = 1, a = 1, b = Inf)

estimativa_I <- mean(C * amostras_x^2)
valor_exato <- dnorm(1)

tabela <- data.frame(
  Metodo = c("Estimativa de Monte Carlo", "Valor Analitico"),
  Valor = c(estimativa_I, valor_exato)
)

kable(tabela, 
      caption = "Metodo para Integral",
      booktabs = T) %>%
    kable_styling(bootstrap_options = c("simple", "hover"), 
                  full_width = F,
                  position = "center")
``` 

# Questão 4

Utilize o algoritmo de Newton-Raphson para estimar os parâmetros de um modelo de regressão Poisson em que $y_{i}\sim Poisson(\lambda=exp\{\beta_{0}+\beta_{1}x_{i}\})$. Gere dados do modelo de regressão e avalie se o algoritmo fornece boas estimativas.

## Solução

Para estimar os parâmetros $\beta_0$ e $\beta_1$ de uma regressão Poisson, precisamos encontrar os valores que maximizam a função de log-verossimilhança $(l(\beta))$. Nesse caso procuramos o ponto onde a primeira derivada de $f(x)$ é igual a zero.

Como o problema é multidimensional (temos dois $\beta$s), nossas ferramentas são:

-   A 1º derivada $g'(x)$: Vetor Score $U(\beta)$ com as derivadas parciais de $l(\beta)$ em relação a cada $\beta$. Nosso objetivo é encontrar o $\beta$ que faz $U(\beta) = 0$.

-   A 2º derivada $g''(x)$: Matriz Hessiana $H(\beta)$. É uma matriz 2x2 com todas as segundas derivadas. 

Usando a fórmula de atualização de Newton-Raphson $$\beta^{(t+1)} = \beta^{(t)} - [H(\beta^{(t)}]^{-1}U(\beta^{(t)})$$. Onde $[H^{-1}]$ é a matriz inversa hessiana.

Para implementar, definimos os componentes do nosso GLM Poisson. O preditor linear é $\eta = X\beta$. A média da Poisson, $\lambda$, é ligada ao preditor pela função de ligação log, $\log(\lambda) = \eta$.

Invertendo a ligação, temos $\lambda = \exp(\eta)$. Esta média $\lambda$ é usada tanto no Vetor Score ($U$) quanto na Matriz Hessiana ($H$). A Hessiana é calculada como $H = -X^T W X$, onde $W$ é uma matriz diagonal de pesos com $W_{ii} = \lambda_i$.

**Algoritmo**

1)  Definir um chute inicial $\beta^{(0)}$

2)  Loop de Iterações:

\- calcular $\eta^{(t)} = X\beta^{(t)}$ e $\lambda^{(t)} = exp(\eta^{(t)})$

\- calcular o **vetor score** $U^{(t)} = X^T(y - \lambda^{(t)})$

\- calcular a **matriz hessiana**:$H^{(t)} = -\mathbf{X}^T \mathbf{W}^{(t)} \mathbf{X}$

\- calcular o passo: $\Delta = [H^{(t)}]^{-1} U^{(t)}$

\- atualizar o chute: $\beta^{(t+1)} = \beta^{(t)} - \Delta$

\- Se a soma das mudanças for menor que a tolerância, paramos o loop.

```{r}
set.seed(42)

n <- 1000 # tamanho da amostra
beta_real <- c(1.5, -0.8) # parâmetros verdadeiros

x <- runif(n, min = -3, max = 3) 
X <- cbind(rep(1, n), x) # Matriz de segundas derivadas

# Calcular o preditor linear e a média 
eta_real <- X %*% beta_real   # eta = beta_0*1 + beta_1*x_i
lambda_real <- exp(eta_real)  # lambda = exp(eta)

# Gerar os dados da distribuição Poisson
y <- rpois(n, lambda = lambda_real)

# Chute inicial para os parâmetros
beta_estimado <- c(0, 0)

tolerancia <- 1e-8      # Critério de parada
max_iteracoes <- 100  # Limite de segurança


for (i in 1:max_iteracoes) {
  
  # Calcular eta e lambda com o beta ATUAL
  eta_atual <- X %*% beta_estimado
  lambda_atual <- exp(eta_atual)
  
  # Calcular o Vetor Score (U)
  score <- t(X) %*% (y - lambda_atual)
  
  # 4c. Calcular a Matriz Hessiana (H)
  W <- diag(as.vector(lambda_atual)) # matriz diagonal de pesos (lambdas)
  hessiana <- -t(X) %*% W %*% X
  
  # Calcular o passo de Newton = H^(-1) * U
 
  passo <- solve(hessiana, score)  # usamos solve(A, b) para calcular A^(-1) * b
  
  # Atualizar os parâmetros: beta_novo = beta_antigo - passo
  beta_novo <- beta_estimado - passo
  
  # Checar a convergência. Se a mudança for muito pequena, paramos o loop
  mudanca <- sum(abs(passo))
  if (mudanca < tolerancia) {
    print(paste("Convergência alcançada na iteração:", i))
    beta_estimado <- beta_novo
    break
  }
  
  # Se não convergiu, atualiza o chute para a próxima rodada
  beta_estimado <- beta_novo
  
}


print("Parâmetros Reais:")
beta_real

print("Parâmetros Estimados (Newton-Raphson):")
as.vector(beta_estimado)

```

### Gráfico para comparação

```{r}
plot(x, y, main = "Regressão Poisson com Newton-Raphson",
     xlab = "Variável X", ylab = "Contagem Y",
     pch = 20, col = "gray",
     ylim = c(0, max(y[x < 0]))) 

# curva real
curve(exp(beta_real[1] + beta_real[2] * x),
      col = "blue", lty = 2, lwd = 3, add = TRUE)

# curva estimada
curve(exp(beta_estimado[1] + beta_estimado[2] * x),
      col = "red", lty = 1, lwd = 3, add = TRUE)

legend("topright",
       legend = c("Dados Gerados", "Curva Real", "Curva Estimada"),
       col = c("gray", "blue", "red"),
       lty = c(NA, 2, 1),
       pch = c(20, NA, NA),
       lwd = c(NA, 3, 3))
```

# Questão 5

Implemente um estudo Monte Carlo com $M=100$ réplicas para avaliar o algoritmo EM implementado nos slides para o modelo Poisson inflado de zeros e avalie as estimativas nos 4 cenários apresentados nos slides.

## Solução

```{r}

```

# Questão 6

Implemente o algoritmo EM para mistura de duas distribuições Normais apresentado nos slides e avalie a estimação em 4 cenários alterando tamanho amostral e valores do parâmetros.

## Solução
Considere uma amostra aleatória \(X_1, \ldots, X_n\) com função densidade de probabilidade dada por:

\[
f(x_i|\theta) = \sum_{k=1}^{K} P(S_i = k) P(X_i = x_i | S_i = k)
= \sum_{k=1}^{K} p_k \, \mathcal{N}(x_i|\mu_k, \sigma_k^2),
\]

onde \(\mathcal{N}(x_i|\mu_k, \sigma_k^2)\) é a densidade normal com média \(\mu_k\) e variância \(\sigma_k^2\), e

\[
\theta = (p_1, \ldots, p_K, \mu_1, \ldots, \mu_K, \sigma_1^2, \ldots, \sigma_K^2).
\]

Em particular, para \(K=2\):

\[
f(x_i|\theta) = p\,\mathcal{N}(x_i|\mu_1, \sigma_1^2) + (1-p)\,\mathcal{N}(x_i|\mu_2, \sigma_2^2).
\]

A função de verossimilhança para dados completos é:

\[
L(\theta|x,s) = \prod_{i=1}^{n}
[p\,\mathcal{N}(x_i|\mu_1,\sigma_1^2)]^{I\{s_i=1\}}
[(1-p)\,\mathcal{N}(x_i|\mu_2,\sigma_2^2)]^{I\{s_i=2\}}.
\]

A função de log-verossimilhança correspondente é:

\[
\begin{aligned}
l(\theta|x,s) &=
\sum_{i=1}^{n} I\{s_i=1\} \left[\ln p - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_1^2 - \frac{(x_i - \mu_1)^2}{2\sigma_1^2}\right] \\
&\quad + \sum_{i=1}^{n} I\{s_i=2\} \left[\ln (1-p) - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_2^2 - \frac{(x_i - \mu_2)^2}{2\sigma_2^2}\right].
\end{aligned}
\]


No passo E, define-se:

\[
\gamma_{i1}^{(t)} = E[I\{S_i=1\} | x_i, \theta^{(t)}] = P(S_i=1|x_i,\theta^{(t)}),
\]
\[
\gamma_{i2}^{(t)} = E[I\{S_i=2\} | x_i, \theta^{(t)}] = P(S_i=2|x_i,\theta^{(t)}).
\]

Assim,

\[
\begin{aligned}
Q(\theta|\theta^{(t)}) &=
\sum_{i=1}^{n} \gamma_{i1}^{(t)} \left[\ln p - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_1^2 - \frac{(x_i - \mu_1)^2}{2\sigma_1^2}\right] \\
&\quad + \gamma_{i2}^{(t)} \left[\ln (1-p) - \tfrac{1}{2}\ln 2\pi - \tfrac{1}{2}\ln \sigma_2^2 - \frac{(x_i - \mu_2)^2}{2\sigma_2^2}\right].
\end{aligned}
\]

No passo M, atualizamos os parâmetros maximizando \(Q(\theta | \theta^{(t)})\):

\[
p^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n}\gamma_{i1}^{(t)}, \quad
1 - p^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n}\gamma_{i2}^{(t)}.
\]

De forma análoga:

\[
\mu_1^{(t+1)} = \frac{\sum_i \gamma_{i1}^{(t)} x_i}{\sum_i \gamma_{i1}^{(t)}}, \quad
\mu_2^{(t+1)} = \frac{\sum_i \gamma_{i2}^{(t)} x_i}{\sum_i \gamma_{i2}^{(t)}},
\]

\[
(\sigma_1^2)^{(t+1)} = \frac{\sum_i \gamma_{i1}^{(t)} (x_i - \mu_1^{(t+1)})^2}{\sum_i \gamma_{i1}^{(t)}}, \quad
(\sigma_2^2)^{(t+1)} = \frac{\sum_i \gamma_{i2}^{(t)} (x_i - \mu_2^{(t+1)})^2}{\sum_i \gamma_{i2}^{(t)}}.
\]

Passo a Passo do Algoritmo:

1. Inicialize \(p^{(0)}, \mu_1^{(0)}, \mu_2^{(0)}, (\sigma_1^2)^{(0)}, (\sigma_2^2)^{(0)}\);
2. Faça \(t = 0\);
3. Repita até convergência:
   - **Passo E:** Calcule \(\gamma_{i1}^{(t)}\) e \(\gamma_{i2}^{(t)}\);
   - **Passo M:** Atualize \(p^{(t+1)}, \mu_1^{(t+1)}, \mu_2^{(t+1)}, (\sigma_1^2)^{(t+1)}, (\sigma_2^2)^{(t+1)}\);
   - Faça \(t \leftarrow t + 1\).


```{r mistura-normais}
mistura_em <- function(x, mu_init, sigma_init, p_init, max_iter = 100, tol = 1e-6) {
  n <- length(x)
  mu <- mu_init     
  sigma <- sigma_init
  p <- p_init
  
  log_veross <- numeric(max_iter)
  
  for (t in 1:max_iter){
    dcomp1 <- p * dnorm(x, mean = mu[1], sd = sigma[1])
    dcomp2 <- (1 - p) * dnorm(x, mean = mu[2], sd = sigma[2])
    
    dtotal <- dcomp1 + dcomp2
  
    
    gamma1 <- dcomp1 / dtotal 
    gamma2 <- dcomp2 / dtotal
    
    log_veross[t] <- sum(log(dtotal))
    
    n1 <- sum(gamma1)
    n2 <- sum(gamma2)
    
    p_cand <- n1 / n
    
    mu1_cand <- sum(gamma1 * x) / n1
    mu2_cand <- sum(gamma2 * x) / n2
    
    sigma1_sq_cand <- sum(gamma1 * (x - mu1_cand)^2) / n1
    sigma2_sq_cand <- sum(gamma2 * (x - mu2_cand)^2) / n2

    if (sigma1_sq_cand <= 0 || sigma2_sq_cand <= 0) {
      break
    }
    
    p <- p_cand
    mu <- c(mu1_cand, mu2_cand)
    sigma <- c(sqrt(sigma1_sq_cand), sqrt(sigma2_sq_cand))
    
    if (t > 1 && abs(log_veross[t] - log_veross[t-1]) < tol) {
      log_veross <- log_veross[1:t] 
      break
    }
  }
  
  return(list(
    p = p,
    mu = mu,
    sigma = sigma,
    log_veross = log_veross,
    iter = t,
    gammas = data.frame(gamma1 = gamma1, gamma2 = gamma2)
  ))
}
```

### Cenários:
```{r mistura-normais-construcao-cenarios, message=FALSE, warning=FALSE}
gerar_mistura <- function(n, p_verd, mu_verd, sigma_verd) {
  componente <- sample(1:2, size = n, replace = TRUE, prob = c(p_verd, 1 - p_verd))
  x <- ifelse(componente == 1, 
              rnorm(n, mean = mu_verd[1], sd = sigma_verd[1]),
              rnorm(n, mean = mu_verd[2], sd = sigma_verd[2]))
  return(x)
}

avaliar_cenario <- function(nome_cenario, n, p_verd, mu_verd, sigma_verd) {
  set.seed(123)
  
  x <- gerar_mistura(n, p_verd, mu_verd, sigma_verd)

  mu_init <- quantile(x, c(0.25, 0.75)) 
  sigma_init <- c(sd(x), sd(x))         
  p_init <- 0.5                          
  
  resultado_em <- mistura_em(x, mu_init, sigma_init, p_init)

  ordem_verd <- order(mu_verd)
  mu_verd <- mu_verd[ordem_verd]
  sigma_verd <- sigma_verd[ordem_verd]
  p_verd <- if (ordem_verd[1] == 1) p_verd else (1 - p_verd)

  ordem_est <- order(resultado_em$mu)
  mu_est <- resultado_em$mu[ordem_est]
  sigma_est <- resultado_em$sigma[ordem_est]
  p_est <- if (ordem_est[1] == 1) resultado_em$p else (1 - resultado_em$p)

  tabela_resultados <- data.frame(
    Parametros = c("p", "Mu", "Sigma"),
    Verdadeiro = c(
      sprintf("%.3f", p_verd),
      sprintf("(%.3f, %.3f)", mu_verd[1], mu_verd[2]),
      sprintf("(%.3f, %.3f)", sigma_verd[1], sigma_verd[2])
    ),
    Estimado_EM = c(
      sprintf("%.3f", p_est),
      sprintf("(%.3f, %.3f)", mu_est[1], mu_est[2]),
      sprintf("(%.3f, %.3f)", sigma_est[1], sigma_est[2])
    )
  )
  
  tabela_formatada <- kable(tabela_resultados,
                            booktabs = T) %>%
    kable_styling(bootstrap_options = c("simple", "hover"), 
                  full_width = F,
                  position = "center")
 
  densidade_mistura <- function(x, p, mu, sigma) {
    p * dnorm(x, mu[1], sigma[1]) + (1 - p) * dnorm(x, mu[2], sigma[2])
  }
  
  df_plot <- data.frame(x = x)
  
  p <- ggplot(df_plot, aes(x = x)) +
    geom_histogram(aes(y = ..density..), bins = 50, fill = "lightblue", 
                   color = "black", alpha = 0.7) +
    stat_function(fun = densidade_mistura, args = list(p = p_verd, mu = mu_verd, 
                                                       sigma = sigma_verd),
                  aes(color = "Verdadeira"), size = 1.2) +
    stat_function(fun = densidade_mistura, args = list(p = p_est, mu = mu_est, 
                                                       sigma = sigma_est),
                  aes(color = "Estimada"), size = 1.2, linetype = "dashed") +
    scale_color_manual(name = "Densidade", 
                       values = c("Verdadeira" = "red3", "Estimada" = "dodgerblue4")) +
    labs(title = nome_cenario,
         subtitle = paste("n =", n),
         x = "Valor",
         y = "Densidade") +
    theme_minimal()
  
  print(p)
  
  cat(paste("O algoritmo convergiu em", resultado_em$iter, "iterações"))
  
  return(tabela_formatada)
}
```

**Cenário 1**

- $n = 2000$

- $p = 0.5$

- $\mu = (0, 5)$

- $\sigma = (1, 1)$

```{r mistura-normais-cenario1, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Grande, Bem Separado",
  n = 2000,
  p_verd = 0.5,
  mu_verd = c(0, 5),
  sigma_verd = c(1, 1)
)
```


**Cenário 2**

- $n = 2000$

- $p = 0.5$

- $\mu = (0, 2)$

- $\sigma = (1, 0.8)$

```{r mistura-normais-cenario2, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Grande, Muito Próximo",
  n = 2000,
  p_verd = 0.5,
  mu_verd = c(0, 2),
  sigma_verd = c(1, 0.8)
)
```

**Cenário 3**

- $n = 200$

- $p = 0.5$

- $\mu = (0, 6)$

- $\sigma = (1, 1)$

```{r mistura-normais-cenario3, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Pequena, Bem Separado",
  n = 200,
  p_verd = 0.5,
  mu_verd = c(0, 5),
  sigma_verd = c(1, 1)
)
```


**Cenário 4**

- $n = 200$

- $p = 0.5$

- $\mu = (0, 2)$

- $\sigma = (1, 0.8)$

```{r mistura-normais-cenario4, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Amostra Pequena, Muito Próximo",
  n = 200,
  p_verd = 0.5,
  mu_verd = c(0, 2),
  sigma_verd = c(1, 0.8)
)
```

**Cenário 5**

- $n = 700$

- $p = 0.3$

- $\mu = (0, 5)$

- $\sigma = (1, 1)$

```{r mistura-normais-cenario5, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Bem separado, p Deslabanceado",
  n = 700,
  p_verd = 0.3,
  mu_verd = c(0, 5),
  sigma_verd = c(1, 1)
)
```

**Cenário 6**

- $n = 700$

- $p = 0.3$

- $\mu = (0, 2)$

- $\sigma = (1, 0.8)$

```{r mistura-normais-cenario6, echo=FALSE, message=FALSE, warning=FALSE}
avaliar_cenario(
  nome_cenario = "Muito Próximo, p Deslabanceado",
  n = 700,
  p_verd = 0.3,
  mu_verd = c(0, 2),
  sigma_verd = c(1, 0.8)
)
```

# Questão 7

Implemente o algoritmo ECM para mistura de duas distribuições Gamas apresentado nos slides de variantes do algoritmo EM. Avalie as estimativas gerando dados de alguns cenários com mistura de duas gamas.

## Solução

## Solução

### Passo a passo

1. Encontre um valor para $\alpha$ que satisfaça uma equação envolvendo a função `digama`. Primeiro, tente resolver usando um método de busca (`uniroot`) e, se essa busca falhar, aplique Newton-Raphson, um método iterativo que ajusta o $\alpha$ usando a derivada da função `digama` (`trigama`), garantindo que o valor final seja positivo.

* A função do ECM deve receber os dados observados e os parâmetros iniciais para a mistura. Depois, é necessário ter uma estrutura para registrar a evolução dos parâmetros ao longo das iterações e definir um loop que irá atualizar os parâmetros até atingir convergência ou o número máximo de iterações.

2. Passo E: calcule a probabilidade de cada observação pertencer à primeira ou à segunda componente da mistura usando as densidades atuais das duas Gamas. Não é obrigatório, mas seria uma boa prática limitar essas probabilidades para evitar valores extremos ou divisões por zero. Compute somas ponderadas das observações e das probabilidades para usar na atualização dos parâmetros.

3. Passo CM: primeiro, atualize a proporção da primeira componente ($\delta$) como a média das probabilidades calculadas no passo E. Em segundo lugar, atualize os parâmetros de taxa ($\beta$) para cada componente usando a média ponderada das observações, e finalmente, calcule os valores intermediários que servem de entrada para resolver numericamente os parâmetros de forma ($\alpha$) usando a função criada no passo 1.

4. Estabeleça um critério de convergência (mudança menor que a tolerância) e registre os resultados assim que convergir.

5. Gere os dados da mistura de das Gamas para os diferentes cenários a partir de uma variável indicadora (define a qual componente cada observação pertence).

6. Analise o número de iterações e os valores das estimativas dos parâmetros.

### Implementação do código

```{r ex7}
# solução numérica para alfa
resolver_alfa <- function(C_k, alfa_inicial = 1) {
  fun <- function(a) digamma(a) - C_k
  resultado <- tryCatch({
    uniroot(fun, interval = c(1e-8, 1e6), tol = 1e-8)$root
  }, error = function(e) {
    # newton-raphson
    a <- max(alfa_inicial, 1e-6)
    for (i in 1:100) {
      g <- digamma(a) - C_k
      tg <- trigamma(a)
      if (is.na(g) || is.na(tg) || tg <= 0) break
      a_novo <- a - g / tg
      if (a_novo <= 0) a_novo <- a / 2
      if (abs(a_novo - a) < 1e-8) { a <- a_novo; break }
      a <- a_novo
    }
    a
  })
  as.numeric(resultado)
}


ecm_gama <- function(X, parametros_iniciais, max_iter = 500, tol = 1e-6) {
  
  # parâmetros iniciais
  theta <- as.numeric(parametros_iniciais[1:5])
  names(theta) <- c("delta", "alfa1", "alfa2", "beta1", "beta2")
  n <- length(X)
  historico <- matrix(NA, nrow = max_iter, ncol = 5)
  colnames(historico) <- names(theta)
  
  for (t in 1:max_iter) {
    delta_t  <- theta["delta"]
    alfa1_t <- theta["alfa1"]
    alfa2_t <- theta["alfa2"]
    beta1_t  <- theta["beta1"]
    beta2_t  <- theta["beta2"]
    
    # passo E
    f1 <- dgamma(X, shape = alfa1_t, rate = beta1_t)
    f2 <- dgamma(X, shape = alfa2_t, rate = beta2_t)
    # p_i = P(Z_i = 1 | x_i, theta^(t)), onde Z_i é a variável latente com distribuição Bernoulli
    p_i <- (delta_t * f1) / (delta_t * f1 + (1 - delta_t) * f2)
    # para garantir estabilidade numérica
    p_i <- pmin(pmax(p_i, 1e-10), 1 - 1e-10)
    
    # passo CM1: maximização condicional em relação a delta
    delta_novo <- mean(p_i)
    
    # passo CM2: maximização condicional em relação a alfa1 e beta1
    somatorio_p <- sum(p_i)
    somatorio_x_p <- sum(p_i * X)
    if (somatorio_p > 0) {
      beta1_novo <- alfa1_t * somatorio_p / somatorio_x_p
      C1 <- (sum(p_i * log(X)) / somatorio_p) + log(beta1_novo)
      alfa1_novo <- resolver_alfa(C1, alfa1_t)
    } else {
      beta1_novo <- beta1_t
      alfa1_novo <- alfa1_t
    }
    
    # passo CM3: maximização condicional em relação a alfa2 e beta2
    somatorio_1mp <- n - somatorio_p
    somatorio_x_1mp <- sum(X) - somatorio_x_p
    if (somatorio_1mp > 0) {
      beta2_novo <- alfa2_t * somatorio_1mp / somatorio_x_1mp
      C2 <- (sum((1 - p_i) * log(X)) / somatorio_1mp) + log(beta2_novo)
      alfa2_novo <- resolver_alfa(C2, alfa2_t)
    } else {
      beta2_novo <- beta2_t
      alfa2_novo <- alfa2_t
    }
    
    # atualização dos parâmetros
    theta_novo <- c(delta_novo, alfa1_novo, alfa2_novo, beta1_novo, beta2_novo)
    names(theta_novo) <- names(theta)
    
    # critério de convergência
    crit <- max(abs(theta_novo - theta) / pmax(abs(theta), 1e-8))
    theta <- theta_novo
    historico[t, ] <- theta
    
    if (crit < tol) break
    
  }
  
  list(estimativas = theta, iteracoes = t, historico = historico[1:t, ])
}

# gerando os dados
mistura_gama <- function(n, delta, alfa1, alfa2, beta1, beta2) {
  Z <- rbinom(n, 1, delta)
  rgamma(n, shape = ifelse(Z == 1, alfa1, alfa2),
         rate = ifelse(Z == 1, beta1, beta2))
}

set.seed(123456789)

# cenário 1: mistura bem separada
parametros1 <- c(delta = 0.6, alfa1 = 5, alfa2 = 2, beta1 = 1, beta2 = 0.2)
dados1 <- mistura_gama(500, parametros1["delta"], parametros1["alfa1"],
                        parametros1["alfa2"], parametros1["beta1"], parametros1["beta2"])
resultado1 <- ecm_gama(dados1, c(0.5, 1, 1, 0.5, 0.5))

# cenário 2: mistura com maior influência da segunda distribuição
parametros2 <- c(delta = 0.2, alfa1 = 10, alfa2 = 5, beta1 = 2, beta2 = 0.5)
dados2 <- mistura_gama(1000, parametros2["delta"], parametros2["alfa1"],
                        parametros2["alfa2"], parametros2["beta1"], parametros2["beta2"])
resultado2 <- ecm_gama(dados2, c(0.5, 1, 1, 0.5, 0.5))

# cenário 3: mistura com densidades sobrepostas

parametros3 <- c(delta = 0.5, alfa1 = 2, alfa2 = 3, beta1 = 1, beta2 = 1.2)
dados3 <- mistura_gama(800, parametros3["delta"], parametros3["alfa1"],
                        parametros3["alfa2"], parametros3["beta1"], parametros3["beta2"])
resultado3 <- ecm_gama(dados3, c(0.5, 1, 1, 0.5, 0.5))

resultados <- data.frame(
  Cenário = c("Cenário 1: Separada", "Cenário 2: Desigual", "Cenário 3: Sobreposta"),
  Iterações = c(resultado1$iteracoes, resultado2$iteracoes, resultado3$iteracoes),
  round(rbind(resultado1$estimativas, resultado2$estimativas, resultado3$estimativas), 4)
)
kable(resultados)

plot_componentes <- function(dados, parametros_verdadeiros, parametros_estimados, titulo) {
  x <- seq(0, max(dados), length.out = 500)
  
  # densidades teóricas e estimadas
  df <- data.frame(
    x = x,
    f1_verdadeira = parametros_verdadeiros["delta"] * dgamma(x, parametros_verdadeiros["alfa1"], parametros_verdadeiros["beta1"]),
    f2_verdadeira = (1 - parametros_verdadeiros["delta"]) * dgamma(x, parametros_verdadeiros["alfa2"], parametros_verdadeiros["beta2"]),
    f1_estimada = parametros_estimados["delta"] * dgamma(x, parametros_estimados["alfa1"], parametros_estimados["beta1"]),
    f2_estimada = (1 - parametros_estimados["delta"]) * dgamma(x, parametros_estimados["alfa2"], parametros_estimados["beta2"])
  )
  
  ggplot() +
    geom_histogram(aes(x = dados, y = after_stat(density)),
                   bins = 40, fill = "grey85", color = "white", alpha = 0.6) +
    
    geom_line(data = df, aes(x = x, y = f1_verdadeira, color = "C1 Verdadeira"),
              linetype = "dashed", linewidth = 1) +
    geom_line(data = df, aes(x = x, y = f2_verdadeira, color = "C2 Verdadeira"),
              linetype = "dashed", linewidth = 1) +
    geom_line(data = df, aes(x = x, y = f1_estimada, color = "C1 Estimada"),
              linewidth = 1.1) +
    geom_line(data = df, aes(x = x, y = f2_estimada, color = "C2 Estimada"),
              linewidth = 1.1) +
    
    scale_color_manual(
      name = "Curvas de densidade",
      values = c("C1 Verdadeira" = "blue",
                 "C2 Verdadeira" = "red",
                 "C1 Estimada" = "darkblue",
                 "C2 Estimada" = "darkred")
    ) +
    
    theme_minimal() +
    labs(title = titulo, x = "x", y = "Densidade") +
    theme(plot.title = element_text(size = 12, face = "bold"),
          legend.title = element_text(size = 10),
          legend.text = element_text(size = 9))
}


pairs = c(mfrow=c(1,3))
plot_componentes(dados1, parametros1, resultado1$estimativas, "Cenário 1")
plot_componentes(dados2, parametros2, resultado2$estimativas, "Cenário 2")
plot_componentes(dados3, parametros3, resultado3$estimativas, "Cenário 3")
```